{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "25RbJ_qbeabL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "KmStZgh6g9Zy"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, config_path=\"config/parameters.yaml\", use_cached=True):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        config_path : str\n",
        "            Relative path to the YAML config from project root.\n",
        "        use_cached : bool\n",
        "            If True, will try to reuse data/processed/signals.npy etc.\n",
        "            before reprocessing raw .mat files.\n",
        "        \"\"\"\n",
        "        # Get the absolute path to config file\n",
        "        current_dir = os.getcwd()\n",
        "        project_root = current_dir\n",
        "        config_path = os.path.join(project_root, config_path)\n",
        "\n",
        "        with open(config_path, \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        self.files = self.config[\"dataset\"][\"files\"]\n",
        "        self.segment_length = self.config[\"preprocessing\"][\"segment_length\"]\n",
        "        self.samples_per_class = self.config[\"preprocessing\"][\"samples_per_class\"]\n",
        "        self.project_root = project_root\n",
        "        self.random_state = self.config[\"preprocessing\"][\"random_state\"]\n",
        "        self.use_cached = use_cached\n",
        "\n",
        "    def _load_cached_data(self):\n",
        "        \"\"\"Try to load cached processed data if available.\"\"\"\n",
        "        processed_dir = os.path.join(self.project_root, \"data\", \"processed\")\n",
        "        signals_path = os.path.join(processed_dir, \"signals.npy\")\n",
        "        labels_path = os.path.join(processed_dir, \"labels.npy\")\n",
        "        label_map_path = os.path.join(processed_dir, \"label_map.json\")\n",
        "\n",
        "        if (\n",
        "            os.path.exists(signals_path)\n",
        "            and os.path.exists(labels_path)\n",
        "            and os.path.exists(label_map_path)\n",
        "        ):\n",
        "            print(\"--  Reusing cached processed data from data/processed/ ...\")\n",
        "            X = np.load(signals_path)\n",
        "            y = np.load(labels_path)\n",
        "            with open(label_map_path, \"r\") as f:\n",
        "                label_map_raw = json.load(f)\n",
        "\n",
        "            # JSON turns int keys into strings â†’ convert back\n",
        "            label_map = {int(k): v for k, v in label_map_raw.items()}\n",
        "\n",
        "            print(f\"   Loaded {X.shape[0]} samples from cache\")\n",
        "            print(f\"   Classes: {list(label_map.values())}\")\n",
        "            return X, y, label_map\n",
        "\n",
        "        return None, None, None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and segment all vibration data.\"\"\"\n",
        "        # Try cached version first\n",
        "        if self.use_cached:\n",
        "            X_cached, y_cached, label_map_cached = self._load_cached_data()\n",
        "            if X_cached is not None:\n",
        "                return X_cached, y_cached, label_map_cached\n",
        "\n",
        "        all_signals = []\n",
        "        all_labels = []\n",
        "        label_map = {}\n",
        "\n",
        "        print(\"--  Loading CWRU Bearing Dataset from raw .mat files...\")\n",
        "\n",
        "        for label, (class_name, filename) in enumerate(self.files.items()):\n",
        "            print(f\"   Loading {class_name} from {filename}...\")\n",
        "            filepath = os.path.join(self.project_root, \"data\", \"raw\", filename)\n",
        "\n",
        "            try:\n",
        "                mat_data = loadmat(filepath)\n",
        "\n",
        "                # Extract vibration signal (handle different key names)\n",
        "                signal_keys = [\n",
        "                    key\n",
        "                    for key in mat_data.keys()\n",
        "                    if \"DE_time\" in key or \"X\" in key or \"driven\" in key\n",
        "                ]\n",
        "                if not signal_keys:\n",
        "                    print(f\"     --  No vibration signal found in {filename}\")\n",
        "                    continue\n",
        "\n",
        "                signal_key = signal_keys[0]\n",
        "                signal = mat_data[signal_key].flatten()\n",
        "\n",
        "                # Segment into fixed-length samples\n",
        "                segments = []\n",
        "                for i in range(\n",
        "                    0, len(signal) - self.segment_length, self.segment_length\n",
        "                ):\n",
        "                    segments.append(signal[i : i + self.segment_length])\n",
        "                    if len(segments) >= self.samples_per_class:\n",
        "                        break\n",
        "\n",
        "                all_signals.extend(segments)\n",
        "                all_labels.extend([label] * len(segments))\n",
        "                label_map[label] = class_name\n",
        "                print(f\"     --  Loaded {len(segments)} samples\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"     --  ERROR loading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = np.array(all_signals)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        # Save processed data\n",
        "        self.save_processed_data(X, y, label_map)\n",
        "\n",
        "        print(f\"\\n--  Dataset Summary:\")\n",
        "        print(f\"   Total samples: {len(X)}\")\n",
        "        print(f\"   Number of classes: {len(np.unique(y))}\")\n",
        "        print(f\"   Signal length: {self.segment_length}\")\n",
        "        print(f\"   Classes: {list(label_map.values())}\")\n",
        "\n",
        "        return X, y, label_map\n",
        "\n",
        "    def create_features(self, signals):\n",
        "        \"\"\"Extract features for traditional ML models.\"\"\"\n",
        "        print(\"--  Extracting features for traditional ML...\")\n",
        "\n",
        "        features = []\n",
        "        for signal in signals:\n",
        "            # Time-domain features\n",
        "            mean = np.mean(signal)\n",
        "            std = np.std(signal)\n",
        "            rms = np.sqrt(np.mean(signal**2))\n",
        "            peak_to_peak = np.max(signal) - np.min(signal)\n",
        "            skewness = np.mean((signal - mean) ** 3) / (std**3) if std != 0 else 0\n",
        "            kurtosis = np.mean((signal - mean) ** 4) / (std**4) if std != 0 else 0\n",
        "\n",
        "            feature_vector = [mean, std, rms, peak_to_peak, skewness, kurtosis]\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        feature_names = [\"mean\", \"std\", \"rms\", \"peak_to_peak\", \"skewness\", \"kurtosis\"]\n",
        "        features_array = np.array(features)\n",
        "\n",
        "        return features_array, feature_names\n",
        "\n",
        "    def save_processed_data(self, X, y, label_map):\n",
        "        \"\"\"Save processed data to files.\"\"\"\n",
        "        processed_dir = os.path.join(self.project_root, \"data\", \"processed\")\n",
        "        os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "        np.save(os.path.join(processed_dir, \"signals.npy\"), X)\n",
        "        np.save(os.path.join(processed_dir, \"labels.npy\"), y)\n",
        "\n",
        "        with open(os.path.join(processed_dir, \"label_map.json\"), \"w\") as f:\n",
        "            json.dump(label_map, f, indent=2)\n",
        "\n",
        "        print(\"-- Saved processed data to data/processed/\")\n",
        "\n",
        "    def prepare_splits(self, X, y, X_features=None):\n",
        "        \"\"\"\n",
        "        Prepare train/validation/test splits with CONSISTENT indices\n",
        "        between raw signals and feature matrices.\n",
        "\n",
        "        Uses preprocessing.test_size and preprocessing.val_size from config\n",
        "        as FINAL fractions of the whole dataset.\n",
        "        \"\"\"\n",
        "        random_state = self.random_state\n",
        "        test_size_cfg = float(self.config[\"preprocessing\"][\"test_size\"])\n",
        "        val_size_cfg = float(self.config[\"preprocessing\"][\"val_size\"])\n",
        "\n",
        "        print(\n",
        "            f\"\\n--  Preparing data splits (test={test_size_cfg}, val={val_size_cfg})...\"\n",
        "        )\n",
        "\n",
        "        if test_size_cfg + val_size_cfg >= 1.0:\n",
        "            raise ValueError(\n",
        "                \"test_size + val_size must be < 1.0 in config/preprocessing.\"\n",
        "            )\n",
        "\n",
        "        # First: split into train vs holdout (val+test)\n",
        "        holdout_size = test_size_cfg + val_size_cfg\n",
        "        relative_test_size = test_size_cfg / holdout_size  # within holdout\n",
        "\n",
        "        indices = np.arange(len(y))\n",
        "\n",
        "        # Split indices â€“ ensures consistent splits for both X and X_features\n",
        "        idx_train, idx_hold, y_train, y_hold = train_test_split(\n",
        "            indices,\n",
        "            y,\n",
        "            test_size=holdout_size,\n",
        "            random_state=random_state,\n",
        "            stratify=y,\n",
        "        )\n",
        "\n",
        "        idx_val, idx_test, y_val, y_test = train_test_split(\n",
        "            idx_hold,\n",
        "            y_hold,\n",
        "            test_size=relative_test_size,\n",
        "            random_state=random_state,\n",
        "            stratify=y_hold,\n",
        "        )\n",
        "\n",
        "        # Raw signals for deep learning\n",
        "        X_train_raw = X[idx_train]\n",
        "        X_val_raw = X[idx_val]\n",
        "        X_test_raw = X[idx_test]\n",
        "\n",
        "        # Features for traditional ML (if provided)\n",
        "        if X_features is not None:\n",
        "            X_train_feat = X_features[idx_train]\n",
        "            X_val_feat = X_features[idx_val]\n",
        "            X_test_feat = X_features[idx_test]\n",
        "\n",
        "            print(\n",
        "                f\"   Traditional ML: {X_train_feat.shape[0]} train, \"\n",
        "                f\"{X_val_feat.shape[0]} val, {X_test_feat.shape[0]} test\"\n",
        "            )\n",
        "        else:\n",
        "            X_train_feat = X_val_feat = X_test_feat = None\n",
        "\n",
        "        # Reshape for deep learning (N, L, 1)\n",
        "        X_train_raw = X_train_raw.reshape(X_train_raw.shape[0], X_train_raw.shape[1], 1)\n",
        "        X_val_raw = X_val_raw.reshape(X_val_raw.shape[0], X_val_raw.shape[1], 1)\n",
        "        X_test_raw = X_test_raw.reshape(X_test_raw.shape[0], X_test_raw.shape[1], 1)\n",
        "\n",
        "        print(\n",
        "            f\"   Deep Learning: {X_train_raw.shape[0]} train, \"\n",
        "            f\"{X_val_raw.shape[0]} val, {X_test_raw.shape[0]} test\"\n",
        "        )\n",
        "\n",
        "        splits = {\n",
        "            \"traditional\": {\n",
        "                \"X_train\": X_train_feat,\n",
        "                \"X_val\": X_val_feat,\n",
        "                \"X_test\": X_test_feat,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_val\": y_val,\n",
        "                \"y_test\": y_test,\n",
        "            },\n",
        "            \"deep_learning\": {\n",
        "                \"X_train\": X_train_raw,\n",
        "                \"X_val\": X_val_raw,\n",
        "                \"X_test\": X_test_raw,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_val\": y_val,\n",
        "                \"y_test\": y_test,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4gSrNz5hAiq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-zkWg-4hJr_"
      },
      "source": [
        "## Traditional Machine Learning Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3NMhLpcshNpI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import yaml\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "jbogzKEyhQm4"
      },
      "outputs": [],
      "source": [
        "class TraditionalML:\n",
        "    def __init__(self):\n",
        "        # Get project root\n",
        "        current_dir = os.getcwd()\n",
        "        self.project_root = current_dir\n",
        "\n",
        "        config_path = os.path.join(self.project_root, \"config\", \"parameters.yaml\")\n",
        "        with open(config_path, \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        self.models = {\n",
        "            \"LogisticRegression\": LogisticRegression(\n",
        "                max_iter=self.config[\"models\"][\"traditional\"][\"logistic_regression\"][\n",
        "                    \"max_iter\"\n",
        "                ],\n",
        "                random_state=42,\n",
        "            ),\n",
        "            \"RandomForest\": RandomForestClassifier(\n",
        "                n_estimators=self.config[\"models\"][\"traditional\"][\"random_forest\"][\n",
        "                    \"n_estimators\"\n",
        "                ],\n",
        "                random_state=42,\n",
        "            ),\n",
        "            \"SVM\": SVC(\n",
        "                kernel=self.config[\"models\"][\"traditional\"][\"svm\"][\"kernel\"],\n",
        "                random_state=42,\n",
        "                probability=True,\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train and evaluate all traditional ML models \"\"\"\n",
        "        print(\"\\n-- Training Traditional ML Models...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Check data shapes\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG - X_train shape: {X_train.shape}\")\n",
        "        print(f\"DEBUG - X_test shape: {X_test.shape}\")\n",
        "        print(f\"DEBUG - y_train shape: {y_train.shape}\")\n",
        "        print(f\"DEBUG - y_test shape: {y_test.shape}\")\n",
        "        \"\"\"\n",
        "\n",
        "        if X_train is None or X_test is None:\n",
        "            print(\"-- ERROR: Feature data is None. Check data loading.\")\n",
        "            return {}\n",
        "\n",
        "        # Hyperparameter tuning for RandomForest\n",
        "        try:\n",
        "            self.tune_random_forest(X_train, y_train)\n",
        "        except Exception as e:\n",
        "            print(f\"-- RandomForest tuning failed, using default params: {e}\")\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "\n",
        "            try:\n",
        "                # Train model\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Predictions (hard labels)\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "                # Check prediction shape\n",
        "                #print(f\"DEBUG - y_pred shape: {y_pred.shape}\")\n",
        "                #print(f\"DEBUG - y_test shape: {y_test.shape}\")\n",
        "\n",
        "                # Try to get class probabilities for ROC/AUC\n",
        "                y_proba = None\n",
        "                if hasattr(model, \"predict_proba\"):\n",
        "                    try:\n",
        "                        y_proba = model.predict_proba(X_test)\n",
        "                        print(f\"DEBUG - y_proba shape: {y_proba.shape}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"   -- Could not compute predict_proba for {name}: {e}\")\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                # Store results\n",
        "                self.results[name] = {\n",
        "                    \"model\": model,\n",
        "                    \"accuracy\": accuracy,\n",
        "                    \"predictions\": y_pred,\n",
        "                    \"true_labels\": y_test,\n",
        "                }\n",
        "\n",
        "                print(f\"   -- {name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                # Save model\n",
        "                models_dir = os.path.join(self.project_root, \"models\")\n",
        "                os.makedirs(models_dir, exist_ok=True)\n",
        "                joblib.dump(model, os.path.join(models_dir, f\"{name.lower()}.pkl\"))\n",
        "\n",
        "                # Save probabilities for ROC if available\n",
        "                if y_proba is not None:\n",
        "                    # Use sorted unique labels to define class order\n",
        "                    class_labels = sorted(np.unique(y_train))\n",
        "                    proba_df = pd.DataFrame(\n",
        "                        y_proba,\n",
        "                        columns=[f\"class_{c}\" for c in class_labels],\n",
        "                    )\n",
        "                    proba_df[\"true_label\"] = y_test\n",
        "\n",
        "                    results_dir = os.path.join(self.project_root, \"results\")\n",
        "                    os.makedirs(results_dir, exist_ok=True)\n",
        "                    proba_path = os.path.join(results_dir, f\"{name}_proba.csv\")\n",
        "                    proba_df.to_csv(proba_path, index=False)\n",
        "                    print(f\"   -- Saved probabilities for {name} to {proba_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   -- ERROR training {name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def tune_random_forest(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Simple hyperparameter tuning for RandomForest using GridSearchCV.\n",
        "\n",
        "        Searches over a small grid and updates self.models['RandomForest']\n",
        "        to the best estimator found (based on CV accuracy).\n",
        "        \"\"\"\n",
        "        if \"RandomForest\" not in self.models:\n",
        "            print(\"-- RandomForest not found in models dict; skipping RF tuning.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n-- Hyperparameter tuning for RandomForest (3-fold CV)...\")\n",
        "\n",
        "        rf = self.models[\"RandomForest\"]\n",
        "\n",
        "        param_grid = {\n",
        "            \"n_estimators\": [50, 100, 200],\n",
        "            \"max_depth\": [None, 10, 20],\n",
        "            \"min_samples_split\": [2, 5],\n",
        "            \"min_samples_leaf\": [1, 2],\n",
        "            \"max_features\": [\"sqrt\", \"log2\"],\n",
        "        }\n",
        "\n",
        "        grid = GridSearchCV(\n",
        "            rf,\n",
        "            param_grid=param_grid,\n",
        "            cv=3,\n",
        "            scoring=\"accuracy\",\n",
        "            n_jobs=-1,\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"   -- Best RF params: {grid.best_params_}\")\n",
        "        print(f\"   -- Best CV accuracy: {grid.best_score_:.4f}\")\n",
        "\n",
        "        # Replace the RandomForest model with the best estimator\n",
        "        self.models[\"RandomForest\"] = grid.best_estimator_\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save traditional ML results.\"\"\"\n",
        "        results_dir = os.path.join(self.project_root, \"results\")\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        # Save accuracy comparison\n",
        "        results_df = pd.DataFrame(\n",
        "            {\n",
        "                \"Model\": list(self.results.keys()),\n",
        "                \"Accuracy\": [result[\"accuracy\"] for result in self.results.values()],\n",
        "                \"Type\": \"Traditional ML\",\n",
        "            }\n",
        "        )\n",
        "        results_df.to_csv(\n",
        "            os.path.join(results_dir, \"traditional_ml_results.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        # Save detailed predictions\n",
        "        all_predictions = []\n",
        "        for model_name, result in self.results.items():\n",
        "            for i, (true, pred) in enumerate(\n",
        "                zip(result[\"true_labels\"], result[\"predictions\"])\n",
        "            ):\n",
        "                all_predictions.append(\n",
        "                    {\n",
        "                        \"model\": model_name,\n",
        "                        \"true_label\": true,\n",
        "                        \"predicted_label\": pred,\n",
        "                        \"correct\": true == pred,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        pd.DataFrame(all_predictions).to_csv(\n",
        "            os.path.join(results_dir, \"traditional_predictions.csv\"), index=False\n",
        "        )\n",
        "        print(\"-- Saved traditional ML results to results/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8KxM03BhXJg"
      },
      "source": [
        "## Deep Learning Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_lqlUGghViy",
        "outputId": "7bdf4383-c18f-4893-bf08-f044e8043f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--  Using TensorFlow Keras\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Optional plotting imports (kept in case you want to extend later)\n",
        "import matplotlib.pyplot as plt  # noqa: F401\n",
        "import seaborn as sns  # noqa: F401\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, callbacks\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "    TF_AVAILABLE = True\n",
        "    print(\"--  Using TensorFlow Keras\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        import keras\n",
        "        from keras import layers, models, callbacks\n",
        "        from keras.optimizers import Adam\n",
        "\n",
        "        TF_AVAILABLE = True\n",
        "        print(\"--  Using standalone Keras\")\n",
        "    except ImportError:\n",
        "        TF_AVAILABLE = False\n",
        "        print(\"--  Deep learning libraries not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "LPrEIVjEhhAo"
      },
      "outputs": [],
      "source": [
        "class DeepLearningModels:\n",
        "    def __init__(self):\n",
        "        current_dir = os.getcwd()\n",
        "        self.project_root = current_dir\n",
        "\n",
        "        config_path = os.path.join(self.project_root, \"config\", \"parameters.yaml\")\n",
        "        with open(config_path, \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "        # Deep learning hyperparameters from config\n",
        "        dl_cfg = self.config.get(\"models\", {}).get(\"deep_learning\", {})\n",
        "        self.epochs = int(dl_cfg.get(\"epochs\", 30))\n",
        "        self.batch_size = int(dl_cfg.get(\"batch_size\", 32))\n",
        "        self.learning_rate = float(dl_cfg.get(\"learning_rate\", 0.001))\n",
        "\n",
        "        # Ensure models and results directories exist\n",
        "        os.makedirs(os.path.join(self.project_root, \"models\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.project_root, \"results\"), exist_ok=True)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # DATA PREPROCESSING\n",
        "    # ------------------------------------------------------------------\n",
        "    def preprocess_data(self, X_train, X_val, X_test):\n",
        "        \"\"\"\n",
        "        Preprocess data for deep learning models.\n",
        "\n",
        "        Uses GLOBAL z-score normalization based on the TRAIN set only.\n",
        "        This preserves relative amplitude differences between samples,\n",
        "        which is important for vibration/fault diagnosis.\n",
        "        \"\"\"\n",
        "        print(\"--  Preprocessing data for deep learning...\")\n",
        "\n",
        "        # Ensure float32 and correct shape\n",
        "        X_train = X_train.astype(np.float32)\n",
        "        X_val = X_val.astype(np.float32)\n",
        "        X_test = X_test.astype(np.float32)\n",
        "\n",
        "        # Compute global mean/std on TRAIN ONLY\n",
        "        train_mean = X_train.mean()\n",
        "        train_std = X_train.std()\n",
        "\n",
        "        if train_std < 1e-8:\n",
        "            print(\"âš ï¸ Train std is extremely small; skipping normalization.\")\n",
        "            return X_train, X_val, X_test\n",
        "\n",
        "        print(f\"   Global train mean: {train_mean:.5f}, std: {train_std:.5f}\")\n",
        "\n",
        "        X_train_norm = (X_train - train_mean) / train_std\n",
        "        X_val_norm = (X_val - train_mean) / train_std\n",
        "        X_test_norm = (X_test - train_mean) / train_std\n",
        "\n",
        "        print(\n",
        "            f\"   Data shapes - Train: {X_train_norm.shape}, \"\n",
        "            f\"Val: {X_val_norm.shape}, Test: {X_test_norm.shape}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"   Train range - Min: {X_train_norm.min():.3f}, \"\n",
        "            f\"Max: {X_train_norm.max():.3f}\"\n",
        "        )\n",
        "\n",
        "        return X_train_norm, X_val_norm, X_test_norm\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_1d_cnn(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Create a 1D CNN model for vibration signals.\n",
        "\n",
        "        This is the '1D CNN' mentioned in your project proposal.\n",
        "        \"\"\"\n",
        "        print(\"   Building 1D CNN architecture...\")\n",
        "\n",
        "        model = models.Sequential(\n",
        "            [\n",
        "                layers.Input(shape=input_shape, name=\"input_layer\"),\n",
        "                # Convolutional block 1\n",
        "                layers.Conv1D(16, kernel_size=7, activation=\"relu\", padding=\"same\"),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.MaxPooling1D(2),\n",
        "                # REMOVE Dropout(0.2) here\n",
        "\n",
        "                # Convolutional block 2\n",
        "                layers.Conv1D(32, kernel_size=5, activation=\"relu\", padding=\"same\"),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.MaxPooling1D(2),\n",
        "                # REMOVE Dropout(0.2) here\n",
        "\n",
        "                # Convolutional block 3\n",
        "                layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.GlobalAveragePooling1D(),\n",
        "\n",
        "                # Dense head\n",
        "                layers.Dense(64, activation=\"relu\"),\n",
        "                layers.Dropout(0.3),  # keep this one\n",
        "                layers.Dense(num_classes, activation=\"softmax\"),\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"sparse_categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_lstm(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Create an LSTM model for temporal vibration patterns.\n",
        "\n",
        "        Simplified vs. your previous version to better suit the\n",
        "        relatively small dataset and reduce over-regularization.\n",
        "        \"\"\"\n",
        "        print(\"   Building LSTM architecture...\")\n",
        "\n",
        "        model = models.Sequential(\n",
        "            [\n",
        "                layers.Input(shape=input_shape, name=\"input_layer\"),\n",
        "                layers.LSTM(\n",
        "                    64,\n",
        "                    return_sequences=False,\n",
        "                    dropout=0.0,\n",
        "                    recurrent_dropout=0.0,\n",
        "                    name=\"lstm_1\",\n",
        "                ),\n",
        "                layers.Dense(32, activation=\"relu\", name=\"dense_1\"),\n",
        "                layers.Dropout(0.2, name=\"dropout_1\"),\n",
        "                layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"sparse_categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # TRAINING UTILITIES\n",
        "    # ------------------------------------------------------------------\n",
        "    def get_callbacks(self, model_name):\n",
        "        \"\"\"Get training callbacks for better convergence.\"\"\"\n",
        "        models_dir = os.path.join(self.project_root, \"models\")\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        callbacks_list = [\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor=\"val_accuracy\",\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_accuracy\",\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                filepath=os.path.join(models_dir, f\"best_{model_name}.h5\"),\n",
        "                monitor=\"val_accuracy\",\n",
        "                save_best_only=True,\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        return callbacks_list\n",
        "\n",
        "    def analyze_dataset(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "        \"\"\"Analyze dataset characteristics.\"\"\"\n",
        "        print(\"\\n--  DATASET ANALYSIS:\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        print(f\"   Training samples: {X_train.shape[0]}\")\n",
        "        print(f\"   Validation samples: {X_val.shape[0]}\")\n",
        "        print(f\"   Test samples: {X_test.shape[0]}\")\n",
        "        print(f\"   Input shape: {X_train.shape[1:]}\")\n",
        "        print(f\"   Number of classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "        # Class distribution\n",
        "        train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
        "        val_unique, val_counts = np.unique(y_val, return_counts=True)\n",
        "        test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
        "\n",
        "        print(f\"\\n   Class Distribution:\")\n",
        "        print(f\"     Train: {dict(zip(train_unique, train_counts))}\")\n",
        "        print(f\"     Val:   {dict(zip(val_unique, val_counts))}\")\n",
        "        print(f\"     Test:  {dict(zip(test_unique, test_counts))}\")\n",
        "\n",
        "        # Data statistics\n",
        "        print(f\"\\n   Data Statistics:\")\n",
        "        print(f\"     Train - Min: {X_train.min():.3f}, Max: {X_train.max():.3f}\")\n",
        "        print(f\"     Val   - Min: {X_val.min():.3f}, Max: {X_val.max():.3f}\")\n",
        "        print(f\"     Test  - Min: {X_test.min():.3f}, Max: {X_test.max():.3f}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def train_single_model(self, model, model_name, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train a single model with comprehensive logging.\"\"\"\n",
        "        print(f\"\\n--  Training {model_name}...\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Display model architecture\n",
        "        print(f\"   {model_name} Architecture:\")\n",
        "        model.summary()\n",
        "\n",
        "        # Get callbacks\n",
        "        training_callbacks = self.get_callbacks(model_name)\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=self.epochs,  # from config\n",
        "            batch_size=self.batch_size,  # from config\n",
        "            callbacks=training_callbacks,\n",
        "            verbose=1,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate_model(self, model, model_name, X_test, y_test):\n",
        "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "        print(f\"\\n-- Evaluating {model_name}...\")\n",
        "\n",
        "        # Basic evaluation\n",
        "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Predictions (probabilities and hard labels)\n",
        "        y_pred_proba = model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "        # Additional metrics\n",
        "        test_accuracy_manual = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"   Manual Accuracy: {test_accuracy_manual:.4f}\")\n",
        "\n",
        "        # Save probabilities for ROC/AUC\n",
        "        try:\n",
        "            results_dir = os.path.join(self.project_root, \"results\")\n",
        "            os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "            # Map internal name \"CNN\" to display name \"1D CNN\" for consistency\n",
        "            display_name = \"1D CNN\" if model_name == \"CNN\" else model_name\n",
        "\n",
        "            # Use a file-friendly stem\n",
        "            stem = display_name.replace(\" \", \"_\")\n",
        "            proba_path = os.path.join(results_dir, f\"{stem}_proba.csv\")\n",
        "\n",
        "            class_labels = sorted(np.unique(y_test))\n",
        "            proba_df = pd.DataFrame(\n",
        "                y_pred_proba,\n",
        "                columns=[f\"class_{c}\" for c in class_labels],\n",
        "            )\n",
        "            proba_df[\"true_label\"] = y_test\n",
        "            proba_df.to_csv(proba_path, index=False)\n",
        "            print(f\"   -- Saved probabilities for {display_name} to {proba_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   -- Could not save probabilities for {model_name}: {e}\")\n",
        "\n",
        "        return {\n",
        "            \"model\": model,\n",
        "            \"accuracy\": test_accuracy,\n",
        "            \"predictions\": y_pred,\n",
        "            \"probabilities\": y_pred_proba,\n",
        "            \"true_labels\": y_test,\n",
        "            \"loss\": test_loss,\n",
        "        }\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # MAIN TRAINING ENTRY POINT\n",
        "    # ------------------------------------------------------------------\n",
        "    def train_models(self, X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "        \"\"\"\n",
        "        Main training function for deep learning models.\n",
        "\n",
        "        Trains exactly the models required by your proposal:\n",
        "        - 1D CNN\n",
        "        - LSTM\n",
        "        \"\"\"\n",
        "        if not TF_AVAILABLE:\n",
        "            print(\"--  Deep learning libraries not available. Skipping DL training.\")\n",
        "            return {}\n",
        "\n",
        "        print(\"\\n DEEP LEARNING MODEL TRAINING\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Preprocess data\n",
        "        X_train_processed, X_val_processed, X_test_processed = self.preprocess_data(\n",
        "            X_train, X_val, X_test\n",
        "        )\n",
        "\n",
        "        # Analyze dataset\n",
        "        self.analyze_dataset(\n",
        "            X_train_processed,\n",
        "            y_train,\n",
        "            X_val_processed,\n",
        "            y_val,\n",
        "            X_test_processed,\n",
        "            y_test,\n",
        "        )\n",
        "\n",
        "        num_classes = len(np.unique(y_train))\n",
        "        input_shape = (X_train_processed.shape[1], X_train_processed.shape[2])\n",
        "\n",
        "        # Define models to train (internal keys are short; display names handled later)\n",
        "        models_to_train = {\n",
        "            \"CNN\": self.create_1d_cnn(input_shape, num_classes),\n",
        "            \"LSTM\": self.create_lstm(input_shape, num_classes),\n",
        "        }\n",
        "\n",
        "        trained_models = {}\n",
        "        training_histories = {}\n",
        "\n",
        "\n",
        "        for model_name, model in models_to_train.items():\n",
        "            try:\n",
        "                # Train model\n",
        "                history = self.train_single_model(\n",
        "                    model,\n",
        "                    model_name,\n",
        "                    X_train_processed,\n",
        "                    y_train,\n",
        "                    X_val_processed,\n",
        "                    y_val,\n",
        "                )\n",
        "\n",
        "                # Evaluate model\n",
        "                results = self.evaluate_model(\n",
        "                    model, model_name, X_test_processed, y_test\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                trained_models[model_name] = results\n",
        "                training_histories[model_name] = history.history\n",
        "\n",
        "                # Save model\n",
        "                model_path = os.path.join(\n",
        "                    self.project_root, \"models\", f\"{model_name}_final.h5\"\n",
        "                )\n",
        "                model.save(model_path)\n",
        "                print(f\"-- Saved {model_name} to {model_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"--  Error training {model_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Store final results\n",
        "        self.results = trained_models\n",
        "\n",
        "        # Save training histories\n",
        "        self.save_training_histories(training_histories)\n",
        "\n",
        "        # Generate comprehensive report\n",
        "        self.generate_detailed_report()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # REPORTING\n",
        "    # ------------------------------------------------------------------\n",
        "    def save_training_histories(self, training_histories):\n",
        "        \"\"\"Save training histories for analysis.\"\"\"\n",
        "        results_dir = os.path.join(self.project_root, \"results\")\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        for model_name, history in training_histories.items():\n",
        "            history_df = pd.DataFrame(history)\n",
        "            history_path = os.path.join(\n",
        "                results_dir, f\"{model_name}_training_history.csv\"\n",
        "            )\n",
        "            history_df.to_csv(history_path, index=False)\n",
        "            print(f\"-- Saved {model_name} training history\")\n",
        "\n",
        "    def generate_detailed_report(self):\n",
        "        \"\"\"Generate detailed performance report.\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"--  No results to generate report\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n--  DETAILED PERFORMANCE REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Create results dataframe\n",
        "        results_data = []\n",
        "        for model_name, result in self.results.items():\n",
        "            # Map internal name \"CNN\" to display name \"1D CNN\" for your report\n",
        "            display_name = \"1D CNN\" if model_name == \"CNN\" else model_name\n",
        "\n",
        "            results_data.append(\n",
        "                {\n",
        "                    \"Model\": display_name,\n",
        "                    \"Accuracy\": result[\"accuracy\"],\n",
        "                    \"Loss\": result[\"loss\"],\n",
        "                    \"Type\": \"Deep Learning\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "        results_df = pd.DataFrame(results_data)\n",
        "\n",
        "        # Save to file\n",
        "        results_dir = os.path.join(self.project_root, \"results\")\n",
        "        results_df.to_csv(\n",
        "            os.path.join(results_dir, \"deep_learning_results.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        # Print ranking\n",
        "        ranked_results = results_df.sort_values(\"Accuracy\", ascending=False)\n",
        "        print(\"\\nðŸ† MODEL RANKINGS:\")\n",
        "        for _, row in ranked_results.iterrows():\n",
        "            stars = \"â­\" * min(5, int(row[\"Accuracy\"] * 10))\n",
        "            print(f\"   {row['Model']:20} {row['Accuracy']:.4f} {stars}\")\n",
        "\n",
        "        # Save predictions\n",
        "        all_predictions = []\n",
        "        for model_name, result in self.results.items():\n",
        "            display_name = \"1D CNN\" if model_name == \"CNN\" else model_name\n",
        "            for true, pred in zip(result[\"true_labels\"], result[\"predictions\"]):\n",
        "                all_predictions.append(\n",
        "                    {\n",
        "                        \"model\": display_name,\n",
        "                        \"true_label\": int(true),\n",
        "                        \"predicted_label\": int(pred),\n",
        "                        \"correct\": bool(true == pred),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        predictions_df = pd.DataFrame(all_predictions)\n",
        "        predictions_df.to_csv(\n",
        "            os.path.join(results_dir, \"deep_learning_predictions.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        print(\"-- Saved detailed reports to results/ folder\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"\n",
        "        Save deep learning results to CSV files.\n",
        "\n",
        "        This mirrors TraditionalML.save_results() so the pipeline can safely call:\n",
        "            dl_models.save_results()\n",
        "\n",
        "        It uses the in-memory `self.results` (populated by `train_models`) and\n",
        "        reuses `generate_detailed_report()` which already writes:\n",
        "\n",
        "            - results/deep_learning_results.csv\n",
        "            - results/deep_learning_predictions.csv\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            print(\n",
        "                \"âš ï¸ No in-memory deep learning results found. \"\n",
        "                \"If you already trained models in a previous run, the CSV files \"\n",
        "                \"in results/ are already on disk. \"\n",
        "                \"If this is a fresh run, call train_models(...) before save_results().\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        self.generate_detailed_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4loxrGMQiekB"
      },
      "source": [
        "## Run the whole experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "UdGZ1Ivvii7j"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "f71sYKVMh31G"
      },
      "outputs": [],
      "source": [
        "class PipelineExecutor:\n",
        "    \"\"\"Main pipeline executor with comprehensive error handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.results = {}\n",
        "        self.pipeline_status = {\n",
        "            \"data_loading\": False,\n",
        "            \"traditional_ml\": False,\n",
        "            \"deep_learning\": False,\n",
        "            \"visualization\": False,\n",
        "        }\n",
        "\n",
        "    def print_header(self):\n",
        "        \"\"\"Print pipeline header\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"--  BEARING FAULT DIAGNOSIS PIPELINE - ROBUST EXECUTION\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    def print_step_header(self, step_number, step_name):\n",
        "        \"\"\"Print step header\"\"\"\n",
        "        print(f\"\\n{'â”'*50}\")\n",
        "        print(f\"--  STEP {step_number}: {step_name}\")\n",
        "        print(f\"{'_'*50}\")\n",
        "\n",
        "    def validate_environment(self):\n",
        "        \"\"\"Validate that all required components are available\"\"\"\n",
        "        print(\"\\nðŸ” VALIDATING ENVIRONMENT...\")\n",
        "\n",
        "        # Check required directories\n",
        "        required_dirs = [\"data/raw\", \"config\"]\n",
        "        for dir_path in required_dirs:\n",
        "            if not os.path.exists(dir_path):\n",
        "                print(f\"--  Missing directory: {dir_path}\")\n",
        "                return False\n",
        "            print(f\"--  Directory exists: {dir_path}\")\n",
        "\n",
        "        # Check required files\n",
        "        required_files = [\"config/parameters.yaml\"]\n",
        "        for file_path in required_files:\n",
        "            if not os.path.exists(file_path):\n",
        "                print(f\"--  Missing file: {file_path}\")\n",
        "                return False\n",
        "            print(f\"--  File exists: {file_path}\")\n",
        "\n",
        "        # Check for data files\n",
        "        data_files = os.listdir(\"data/raw\")\n",
        "        if not data_files:\n",
        "            print(\"--  No data files found in data/raw/\")\n",
        "            print(\"   Please download CWRU .mat files first\")\n",
        "            return False\n",
        "\n",
        "        print(f\"--  Found {len(data_files)} data files in data/raw/\")\n",
        "        return True\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Step 1: Data loading and preprocessing\"\"\"\n",
        "        self.print_step_header(1, \"DATA LOADING AND PREPROCESSING\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            print(\"--  Initializing data loader...\")\n",
        "            loader = DataLoader(use_cached=True)  # or False to force reprocessing\n",
        "\n",
        "            print(\"--  Loading and segmenting vibration data...\")\n",
        "            X, y, label_map = loader.load_data()\n",
        "\n",
        "            if len(X) == 0:\n",
        "                raise ValueError(\"No data loaded - check your .mat files\")\n",
        "\n",
        "            print(\"--  Extracting features for traditional ML...\")\n",
        "            features, feature_names = loader.create_features(X)\n",
        "\n",
        "            print(\"--  Preparing train/validation/test splits...\")\n",
        "            splits = loader.prepare_splits(X, y, features)\n",
        "\n",
        "            # Store results\n",
        "            self.results[\"data\"] = {\n",
        "                \"X\": X,\n",
        "                \"y\": y,\n",
        "                \"label_map\": label_map,\n",
        "                \"features\": features,\n",
        "                \"feature_names\": feature_names,\n",
        "                \"splits\": splits,\n",
        "            }\n",
        "\n",
        "            self.pipeline_status[\"data_loading\"] = True\n",
        "            print(\"--  Data loading completed successfully!\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Data loading failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_traditional_ml(self):\n",
        "        \"\"\"Step 2: Traditional Machine Learning\"\"\"\n",
        "        self.print_step_header(2, \"TRADITIONAL MACHINE LEARNING\")\n",
        "\n",
        "        try:\n",
        "            if not self.pipeline_status[\"data_loading\"]:\n",
        "                raise ValueError(\"Data not loaded - run Step 1 first\")\n",
        "\n",
        "            data = self.results[\"data\"]\n",
        "            splits = data[\"splits\"]\n",
        "\n",
        "            print(\"--  Initializing traditional ML models...\")\n",
        "            traditional_ml = TraditionalML()\n",
        "\n",
        "            print(\"--  Training Logistic Regression, Random Forest, and SVM...\")\n",
        "            traditional_results = traditional_ml.train_and_evaluate(\n",
        "                splits[\"traditional\"][\"X_train\"],\n",
        "                splits[\"traditional\"][\"X_test\"],\n",
        "                splits[\"traditional\"][\"y_train\"],\n",
        "                splits[\"traditional\"][\"y_test\"],\n",
        "            )\n",
        "\n",
        "            if not traditional_results:\n",
        "                raise ValueError(\"Traditional ML training returned no results\")\n",
        "\n",
        "            print(\"-- Saving traditional ML results...\")\n",
        "            traditional_ml.save_results()\n",
        "\n",
        "            self.results[\"traditional_ml\"] = traditional_results\n",
        "            self.pipeline_status[\"traditional_ml\"] = True\n",
        "\n",
        "            print(\"--  Traditional ML training completed successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Traditional ML training failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_deep_learning(self):\n",
        "        \"\"\"Step 3: Deep Learning Models\"\"\"\n",
        "        self.print_step_header(3, \"DEEP LEARNING MODELS\")\n",
        "\n",
        "        try:\n",
        "            if not TF_AVAILABLE:\n",
        "                print(\"âš ï¸  Deep learning libraries not available. Skipping deep learning step...\")\n",
        "                self.pipeline_status[\"deep_learning\"] = True\n",
        "                return True\n",
        "\n",
        "            if not self.pipeline_status[\"data_loading\"]:\n",
        "                raise ValueError(\"Data not loaded - run Step 1 first\")\n",
        "\n",
        "            data = self.results[\"data\"]\n",
        "            splits = data[\"splits\"]\n",
        "\n",
        "            print(\"ðŸ§  Initializing deep learning models...\")\n",
        "            dl_models = DeepLearningModels()\n",
        "\n",
        "            print(\"--  Training CNN and LSTM models...\")\n",
        "            dl_results = dl_models.train_models(\n",
        "                splits[\"deep_learning\"][\"X_train\"],\n",
        "                splits[\"deep_learning\"][\"X_val\"],\n",
        "                splits[\"deep_learning\"][\"X_test\"],\n",
        "                splits[\"deep_learning\"][\"y_train\"],\n",
        "                splits[\"deep_learning\"][\"y_val\"],\n",
        "                splits[\"deep_learning\"][\"y_test\"],\n",
        "            )\n",
        "\n",
        "            print(\"-- Saving deep learning results...\")\n",
        "            dl_models.save_results()\n",
        "\n",
        "            self.results[\"deep_learning\"] = dl_results\n",
        "            self.pipeline_status[\"deep_learning\"] = True\n",
        "\n",
        "            print(\"--  Deep learning training completed successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Deep learning training failed: {e}\")\n",
        "            # Don't fail the whole pipeline if DL fails\n",
        "            self.pipeline_status[\"deep_learning\"] = True\n",
        "            return True\n",
        "\n",
        "    def generate_results_summary(self):\n",
        "        \"\"\"Step 4: Results Analysis and Summary\"\"\"\n",
        "        self.print_step_header(4, \"RESULTS ANALYSIS AND SUMMARY\")\n",
        "\n",
        "        try:\n",
        "            print(\"ðŸ“ˆ Generating comprehensive results summary...\")\n",
        "\n",
        "            # Load and combine all available results\n",
        "            all_results = []\n",
        "\n",
        "            # Traditional ML results\n",
        "            try:\n",
        "                trad_results = pd.read_csv(\"results/traditional_ml_results.csv\")\n",
        "                all_results.append(trad_results)\n",
        "                print(\"--  Loaded traditional ML results\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"âš ï¸  Traditional ML results not found\")\n",
        "\n",
        "            # Deep Learning results\n",
        "            try:\n",
        "                dl_results = pd.read_csv(\"results/deep_learning_results.csv\")\n",
        "                all_results.append(dl_results)\n",
        "                print(\"--  Loaded deep learning results\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"âš ï¸  Deep learning results not found\")\n",
        "\n",
        "            if not all_results:\n",
        "                print(\"--  No results found to generate summary\")\n",
        "                return False\n",
        "\n",
        "            # Combine all results\n",
        "            combined_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "            # Generate comprehensive summary\n",
        "            self._print_detailed_summary(combined_results)\n",
        "\n",
        "            # Save combined results\n",
        "            combined_results.to_csv(\"results/combined_results.csv\", index=False)\n",
        "\n",
        "            self.pipeline_status[\"visualization\"] = True\n",
        "            print(\"--  Results summary completed successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Results summary generation failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _print_detailed_summary(self, results_df):\n",
        "        \"\"\"Print detailed results summary\"\"\"\n",
        "        print(\"\\n\" + \"--  COMPREHENSIVE PERFORMANCE REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Sort by accuracy\n",
        "        ranked_results = results_df.sort_values(\"Accuracy\", ascending=False)\n",
        "\n",
        "        print(\"\\nðŸ† MODEL RANKINGS:\")\n",
        "        print(\"-\" * 40)\n",
        "        for idx, (_, row) in enumerate(ranked_results.iterrows(), 1):\n",
        "            accuracy_percent = row[\"Accuracy\"] * 100\n",
        "            stars = \"â­\" * min(5, int(row[\"Accuracy\"] * 10 // 2))\n",
        "            rank_icon = [\"ðŸ¥‡\", \"ðŸ¥ˆ\", \"ðŸ¥‰\"][idx - 1] if idx <= 3 else f\"{idx:2d}\"\n",
        "\n",
        "            print(f\"   {rank_icon} {row['Model']:20} {accuracy_percent:6.2f}% {stars}\")\n",
        "\n",
        "        # Best model\n",
        "        best_model = ranked_results.iloc[0]\n",
        "        print(f\"\\n--  BEST PERFORMING MODEL:\")\n",
        "        print(f\"   Model:    {best_model['Model']}\")\n",
        "        print(\n",
        "            f\"   Accuracy: {best_model['Accuracy']:.4f} ({best_model['Accuracy']*100:.2f}%)\"\n",
        "        )\n",
        "        print(f\"   Type:     {best_model['Type']}\")\n",
        "\n",
        "        # Statistics\n",
        "        print(f\"\\nðŸ“ˆ PERFORMANCE STATISTICS:\")\n",
        "        print(f\"   Total Models:    {len(results_df)}\")\n",
        "        print(f\"   Average Accuracy: {results_df['Accuracy'].mean():.4f}\")\n",
        "        print(f\"   Best Accuracy:    {results_df['Accuracy'].max():.4f}\")\n",
        "        print(f\"   Worst Accuracy:   {results_df['Accuracy'].min():.4f}\")\n",
        "\n",
        "        # Model type breakdown\n",
        "        model_types = results_df[\"Type\"].value_counts()\n",
        "        print(f\"\\n--  MODEL TYPE BREAKDOWN:\")\n",
        "        for model_type, count in model_types.items():\n",
        "            type_accuracy = results_df[results_df[\"Type\"] == model_type][\n",
        "                \"Accuracy\"\n",
        "            ].mean()\n",
        "            print(f\"   {model_type:20} {count:2d} models, avg: {type_accuracy:.4f}\")\n",
        "\n",
        "    def calculate_execution_time(self):\n",
        "        \"\"\"Calculate and format execution time\"\"\"\n",
        "        if self.start_time:\n",
        "            end_time = time.time()\n",
        "            total_seconds = end_time - self.start_time\n",
        "            minutes = int(total_seconds // 60)\n",
        "            seconds = int(total_seconds % 60)\n",
        "            return minutes, seconds\n",
        "        return 0, 0\n",
        "\n",
        "    def print_final_summary(self):\n",
        "        \"\"\"Print final pipeline summary\"\"\"\n",
        "        minutes, seconds = self.calculate_execution_time()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ðŸŽ‰ PIPELINE EXECUTION COMPLETED!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Pipeline status\n",
        "        print(\"\\n--  PIPELINE STATUS:\")\n",
        "        for step, status in self.pipeline_status.items():\n",
        "            status_icon = \"-- \" if status else \"-- \"\n",
        "            step_name = step.replace(\"_\", \" \").title()\n",
        "            print(f\"   {status_icon} {step_name}\")\n",
        "\n",
        "        # Data summary\n",
        "        if self.pipeline_status[\"data_loading\"]:\n",
        "            data = self.results[\"data\"]\n",
        "            print(f\"\\n--  DATA SUMMARY:\")\n",
        "            print(f\"   Samples: {len(data['X'])}\")\n",
        "            print(f\"   Classes: {len(data['label_map'])}\")\n",
        "            print(f\"   Signal Length: {data['X'].shape[1]}\")\n",
        "\n",
        "        # Results summary\n",
        "        print(f\"\\nâ±ï¸  EXECUTION TIME:\")\n",
        "        print(f\"   Total: {minutes} minutes {seconds} seconds\")\n",
        "\n",
        "        print(f\"\\n-- OUTPUTS GENERATED:\")\n",
        "        output_dirs = [\"results\", \"models\", \"data/processed\"]\n",
        "        for dir_path in output_dirs:\n",
        "            if os.path.exists(dir_path) and os.listdir(dir_path):\n",
        "                file_count = len(os.listdir(dir_path))\n",
        "                print(f\"   {dir_path}: {file_count} files\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Execute the complete pipeline\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.print_header()\n",
        "\n",
        "        # Validate environment first\n",
        "        if not self.validate_environment():\n",
        "            print(\"--  Environment validation failed. Please check setup.\")\n",
        "            return False\n",
        "\n",
        "        # Execute pipeline steps\n",
        "        steps = [\n",
        "            self.load_data,\n",
        "            self.train_traditional_ml,\n",
        "            self.train_deep_learning,\n",
        "            self.generate_results_summary,\n",
        "        ]\n",
        "\n",
        "        successful_steps = 0\n",
        "        for step_func in steps:\n",
        "            try:\n",
        "                if step_func():\n",
        "                    successful_steps += 1\n",
        "            except Exception as e:\n",
        "                print(f\"--  Step failed with exception: {e}\")\n",
        "                traceback.print_exc()\n",
        "                # Continue with next step instead of failing completely\n",
        "\n",
        "        # Final summary\n",
        "        self.print_final_summary()\n",
        "\n",
        "        # Success criteria - at least data loading and traditional ML should work\n",
        "        if successful_steps >= 2:\n",
        "            print(\"--  Pipeline completed with acceptable success!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âš ï¸  Pipeline completed with limited success.\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hLEb15M4iVO_",
        "outputId": "a1f89586-1684-4280-90bd-f2fbe7a85514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "--  BEARING FAULT DIAGNOSIS PIPELINE - ROBUST EXECUTION\n",
            "======================================================================\n",
            "Start Time: 2025-11-30 02:38:50\n",
            "======================================================================\n",
            "\n",
            "ðŸ” VALIDATING ENVIRONMENT...\n",
            "--  Directory exists: data/raw\n",
            "--  Directory exists: config\n",
            "--  File exists: config/parameters.yaml\n",
            "--  Found 6 data files in data/raw/\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "--  STEP 1: DATA LOADING AND PREPROCESSING\n",
            "__________________________________________________\n",
            "--  Initializing data loader...\n",
            "--  Loading and segmenting vibration data...\n",
            "--  Loading CWRU Bearing Dataset from raw .mat files...\n",
            "   Loading Normal from 97.mat...\n",
            "     --  Loaded 200 samples\n",
            "   Loading Ball_007_0 from 118.mat...\n",
            "     --  Loaded 119 samples\n",
            "   Loading Ball_021_0 from 222.mat...\n",
            "     --  Loaded 119 samples\n",
            "   Loading Inner_Race_007_0 from 105.mat...\n",
            "     --  Loaded 118 samples\n",
            "   Loading Inner_Race_021_0 from 209.mat...\n",
            "     --  Loaded 119 samples\n",
            "   Loading Outer_Race_007_0 from 130.mat...\n",
            "     --  Loaded 119 samples\n",
            "-- Saved processed data to data/processed/\n",
            "\n",
            "--  Dataset Summary:\n",
            "   Total samples: 794\n",
            "   Number of classes: 6\n",
            "   Signal length: 1024\n",
            "   Classes: ['Normal', 'Ball_007_0', 'Ball_021_0', 'Inner_Race_007_0', 'Inner_Race_021_0', 'Outer_Race_007_0']\n",
            "--  Extracting features for traditional ML...\n",
            "--  Extracting features for traditional ML...\n",
            "--  Preparing train/validation/test splits...\n",
            "\n",
            "--  Preparing data splits (test=0.2, val=0.1)...\n",
            "   Traditional ML: 555 train, 79 val, 160 test\n",
            "   Deep Learning: 555 train, 79 val, 160 test\n",
            "--  Data loading completed successfully!\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "--  STEP 2: TRADITIONAL MACHINE LEARNING\n",
            "__________________________________________________\n",
            "--  Initializing traditional ML models...\n",
            "--  Training Logistic Regression, Random Forest, and SVM...\n",
            "\n",
            "-- Training Traditional ML Models...\n",
            "==================================================\n",
            "\n",
            "-- Hyperparameter tuning for RandomForest (3-fold CV)...\n",
            "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
            "   -- Best RF params: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "   -- Best CV accuracy: 0.9964\n",
            "\n",
            "Training LogisticRegression...\n",
            "DEBUG - y_proba shape: (160, 6)\n",
            "   -- LogisticRegression Accuracy: 0.9375\n",
            "   -- Saved probabilities for LogisticRegression to /Users/blongho/dev/eel5825-project/results/LogisticRegression_proba.csv\n",
            "\n",
            "Training RandomForest...\n",
            "DEBUG - y_proba shape: (160, 6)\n",
            "   -- RandomForest Accuracy: 1.0000\n",
            "   -- Saved probabilities for RandomForest to /Users/blongho/dev/eel5825-project/results/RandomForest_proba.csv\n",
            "\n",
            "Training SVM...\n",
            "DEBUG - y_proba shape: (160, 6)\n",
            "   -- SVM Accuracy: 0.8875\n",
            "   -- Saved probabilities for SVM to /Users/blongho/dev/eel5825-project/results/SVM_proba.csv\n",
            "-- Saving traditional ML results...\n",
            "-- Saved traditional ML results to results/\n",
            "--  Traditional ML training completed successfully!\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "--  STEP 3: DEEP LEARNING MODELS\n",
            "__________________________________________________\n",
            "ðŸ§  Initializing deep learning models...\n",
            "--  Training CNN and LSTM models...\n",
            "\n",
            " DEEP LEARNING MODEL TRAINING\n",
            "==================================================\n",
            "--  Preprocessing data for deep learning...\n",
            "   Global train mean: 0.01576, std: 0.35786\n",
            "   Data shapes - Train: (555, 1024, 1), Val: (79, 1024, 1), Test: (160, 1024, 1)\n",
            "   Train range - Min: -9.569, Max: 10.541\n",
            "\n",
            "--  DATASET ANALYSIS:\n",
            "========================================\n",
            "   Training samples: 555\n",
            "   Validation samples: 79\n",
            "   Test samples: 160\n",
            "   Input shape: (1024, 1)\n",
            "   Number of classes: 6\n",
            "\n",
            "   Class Distribution:\n",
            "     Train: {0: 140, 1: 83, 2: 83, 3: 83, 4: 83, 5: 83}\n",
            "     Val:   {0: 20, 1: 12, 2: 12, 3: 11, 4: 12, 5: 12}\n",
            "     Test:  {0: 40, 1: 24, 2: 24, 3: 24, 4: 24, 5: 24}\n",
            "\n",
            "   Data Statistics:\n",
            "     Train - Min: -9.569, Max: 10.541\n",
            "     Val   - Min: -8.935, Max: 10.333\n",
            "     Test  - Min: -8.947, Max: 9.833\n",
            "   Building 1D CNN architecture...\n",
            "   Building LSTM architecture...\n",
            "\n",
            "--  Training CNN...\n",
            "----------------------------------------\n",
            "   CNN Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv1d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_18          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,592</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_19          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_20          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling1d_6      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv1d_18 (\u001b[38;5;33mConv1D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m16\u001b[0m)       â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_18          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m16\u001b[0m)       â”‚            \u001b[38;5;34m64\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_12 (\u001b[38;5;33mMaxPooling1D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m16\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_19 (\u001b[38;5;33mConv1D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚         \u001b[38;5;34m2,592\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_19          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_13 (\u001b[38;5;33mMaxPooling1D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_20 (\u001b[38;5;33mConv1D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚         \u001b[38;5;34m6,208\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_20          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ global_average_pooling1d_6      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m390\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,926</span> (54.40 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,926\u001b[0m (54.40 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,702</span> (53.52 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,702\u001b[0m (53.52 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4831 - loss: 1.3851\n",
            "Epoch 1: val_accuracy improved from None to 0.30380, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.6559 - loss: 1.1016 - val_accuracy: 0.3038 - val_loss: 1.6847 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8787 - loss: 0.6059\n",
            "Epoch 2: val_accuracy improved from 0.30380 to 0.59494, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8847 - loss: 0.5529 - val_accuracy: 0.5949 - val_loss: 1.5627 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9367 - loss: 0.3224\n",
            "Epoch 3: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9243 - loss: 0.3060 - val_accuracy: 0.4557 - val_loss: 1.4940 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9462 - loss: 0.2166\n",
            "Epoch 4: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9423 - loss: 0.1987 - val_accuracy: 0.4557 - val_loss: 1.4800 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9662 - loss: 0.1373\n",
            "Epoch 5: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9676 - loss: 0.1253 - val_accuracy: 0.4430 - val_loss: 1.4565 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9821 - loss: 0.0997\n",
            "Epoch 6: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9856 - loss: 0.0947 - val_accuracy: 0.4051 - val_loss: 1.4550 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9916 - loss: 0.0751\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9910 - loss: 0.0720 - val_accuracy: 0.4557 - val_loss: 1.3774 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9924 - loss: 0.0518\n",
            "Epoch 8: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9946 - loss: 0.0495 - val_accuracy: 0.4557 - val_loss: 1.3280 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9972 - loss: 0.0441\n",
            "Epoch 9: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9964 - loss: 0.0410 - val_accuracy: 0.5443 - val_loss: 1.2258 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9994 - loss: 0.0264\n",
            "Epoch 10: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9982 - loss: 0.0300 - val_accuracy: 0.5443 - val_loss: 1.1076 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0306\n",
            "Epoch 11: val_accuracy did not improve from 0.59494\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0274 - val_accuracy: 0.5823 - val_loss: 0.9334 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9994 - loss: 0.0204\n",
            "Epoch 12: val_accuracy improved from 0.59494 to 0.60759, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9964 - loss: 0.0316 - val_accuracy: 0.6076 - val_loss: 0.8085 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0300\n",
            "Epoch 13: val_accuracy improved from 0.60759 to 0.62025, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0277 - val_accuracy: 0.6203 - val_loss: 0.7463 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9997 - loss: 0.0210\n",
            "Epoch 14: val_accuracy improved from 0.62025 to 0.78481, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9982 - loss: 0.0231 - val_accuracy: 0.7848 - val_loss: 0.5134 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0194\n",
            "Epoch 15: val_accuracy improved from 0.78481 to 0.84810, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0189 - val_accuracy: 0.8481 - val_loss: 0.3936 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0200\n",
            "Epoch 16: val_accuracy improved from 0.84810 to 0.92405, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 0.9241 - val_loss: 0.2546 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0142\n",
            "Epoch 17: val_accuracy improved from 0.92405 to 0.94937, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 0.9494 - val_loss: 0.1628 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9992 - loss: 0.0155\n",
            "Epoch 18: val_accuracy improved from 0.94937 to 0.97468, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9982 - loss: 0.0143 - val_accuracy: 0.9747 - val_loss: 0.1152 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0106\n",
            "Epoch 19: val_accuracy did not improve from 0.97468\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0131 - val_accuracy: 0.9747 - val_loss: 0.0850 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0081\n",
            "Epoch 20: val_accuracy improved from 0.97468 to 0.98734, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0088 - val_accuracy: 0.9873 - val_loss: 0.0596 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0096\n",
            "Epoch 21: val_accuracy did not improve from 0.98734\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0111 - val_accuracy: 0.9367 - val_loss: 0.1155 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0142\n",
            "Epoch 22: val_accuracy improved from 0.98734 to 1.00000, saving model to /Users/blongho/dev/eel5825-project/models/best_CNN.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0389 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0064\n",
            "Epoch 23: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 0.0406 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0111\n",
            "Epoch 24: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 0.0263 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9955 - loss: 0.0146\n",
            "Epoch 25: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9982 - loss: 0.0107 - val_accuracy: 0.9873 - val_loss: 0.0324 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0074\n",
            "Epoch 26: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9946 - loss: 0.0328 - val_accuracy: 0.9747 - val_loss: 0.0786 - learning_rate: 5.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9927 - loss: 0.0293\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 27: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9928 - loss: 0.0314 - val_accuracy: 0.8861 - val_loss: 0.1895 - learning_rate: 5.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0107\n",
            "Epoch 28: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.8861 - val_loss: 0.2097 - learning_rate: 2.5000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0050\n",
            "Epoch 29: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.8481 - val_loss: 0.2298 - learning_rate: 2.5000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m17/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9991 - loss: 0.0096\n",
            "Epoch 30: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9982 - loss: 0.0123 - val_accuracy: 0.9747 - val_loss: 0.0467 - learning_rate: 2.5000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0069\n",
            "Epoch 31: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0104 - learning_rate: 2.5000e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0073\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 32: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 1.0000 - val_loss: 0.0127 - learning_rate: 2.5000e-04\n",
            "Epoch 32: early stopping\n",
            "Restoring model weights from the end of the best epoch: 22.\n",
            "\n",
            "-- Evaluating CNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Test Loss: 0.0289\n",
            "   Test Accuracy: 1.0000\n",
            "   Manual Accuracy: 1.0000\n",
            "   -- Saved probabilities for 1D CNN to /Users/blongho/dev/eel5825-project/results/1D_CNN_proba.csv\n",
            "-- Saved CNN to /Users/blongho/dev/eel5825-project/models/CNN_final.h5\n",
            "\n",
            "--  Training LSTM...\n",
            "----------------------------------------\n",
            "   LSTM Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m16,896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ output_layer (\u001b[38;5;33mDense\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m198\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,174</span> (74.90 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,174\u001b[0m (74.90 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,174</span> (74.90 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,174\u001b[0m (74.90 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.2104 - loss: 1.7913\n",
            "Epoch 1: val_accuracy improved from None to 0.26582, saving model to /Users/blongho/dev/eel5825-project/models/best_LSTM.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 356ms/step - accuracy: 0.2486 - loss: 1.7867 - val_accuracy: 0.2658 - val_loss: 1.7804 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - accuracy: 0.2840 - loss: 1.7700\n",
            "Epoch 2: val_accuracy improved from 0.26582 to 0.31646, saving model to /Users/blongho/dev/eel5825-project/models/best_LSTM.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 489ms/step - accuracy: 0.2991 - loss: 1.7626 - val_accuracy: 0.3165 - val_loss: 1.7062 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step - accuracy: 0.3343 - loss: 1.6890\n",
            "Epoch 3: val_accuracy did not improve from 0.31646\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 385ms/step - accuracy: 0.3243 - loss: 1.7121 - val_accuracy: 0.2785 - val_loss: 1.6522 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step - accuracy: 0.3240 - loss: 1.5388\n",
            "Epoch 4: val_accuracy did not improve from 0.31646\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 392ms/step - accuracy: 0.3225 - loss: 1.5394 - val_accuracy: 0.2405 - val_loss: 1.6621 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step - accuracy: 0.2850 - loss: 1.6194\n",
            "Epoch 5: val_accuracy improved from 0.31646 to 0.35443, saving model to /Users/blongho/dev/eel5825-project/models/best_LSTM.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 400ms/step - accuracy: 0.3117 - loss: 1.5918 - val_accuracy: 0.3544 - val_loss: 1.4673 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - accuracy: 0.3302 - loss: 1.5895\n",
            "Epoch 6: val_accuracy improved from 0.35443 to 0.39241, saving model to /Users/blongho/dev/eel5825-project/models/best_LSTM.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 352ms/step - accuracy: 0.3586 - loss: 1.5199 - val_accuracy: 0.3924 - val_loss: 1.3856 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - accuracy: 0.3269 - loss: 1.4144\n",
            "Epoch 7: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 431ms/step - accuracy: 0.3658 - loss: 1.3613 - val_accuracy: 0.3924 - val_loss: 1.2810 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.3442 - loss: 1.3531\n",
            "Epoch 8: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 495ms/step - accuracy: 0.3784 - loss: 1.3469 - val_accuracy: 0.3418 - val_loss: 1.2969 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - accuracy: 0.3961 - loss: 1.3101\n",
            "Epoch 9: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 522ms/step - accuracy: 0.3946 - loss: 1.3189 - val_accuracy: 0.3671 - val_loss: 1.2909 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.3883 - loss: 1.2889\n",
            "Epoch 10: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 316ms/step - accuracy: 0.3946 - loss: 1.2957 - val_accuracy: 0.3544 - val_loss: 1.2894 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.3906 - loss: 1.3132\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 468ms/step - accuracy: 0.3946 - loss: 1.3070 - val_accuracy: 0.3671 - val_loss: 1.2853 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512ms/step - accuracy: 0.3694 - loss: 1.3167\n",
            "Epoch 12: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 527ms/step - accuracy: 0.3856 - loss: 1.3062 - val_accuracy: 0.3291 - val_loss: 1.2858 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.3779 - loss: 1.3230\n",
            "Epoch 13: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 327ms/step - accuracy: 0.3712 - loss: 1.3082 - val_accuracy: 0.3418 - val_loss: 1.2870 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4093 - loss: 1.2741\n",
            "Epoch 14: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 250ms/step - accuracy: 0.3964 - loss: 1.2930 - val_accuracy: 0.3797 - val_loss: 1.2845 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.3975 - loss: 1.2949\n",
            "Epoch 15: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.3784 - loss: 1.3042 - val_accuracy: 0.3418 - val_loss: 1.2540 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.3933 - loss: 1.2636\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 16: val_accuracy did not improve from 0.39241\n",
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 345ms/step - accuracy: 0.3676 - loss: 1.2957 - val_accuracy: 0.3544 - val_loss: 1.2502 - learning_rate: 5.0000e-04\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "\n",
            "-- Evaluating LSTM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Test Loss: 1.3780\n",
            "   Test Accuracy: 0.3938\n",
            "   Manual Accuracy: 0.3937\n",
            "   -- Saved probabilities for LSTM to /Users/blongho/dev/eel5825-project/results/LSTM_proba.csv\n",
            "-- Saved LSTM to /Users/blongho/dev/eel5825-project/models/LSTM_final.h5\n",
            "-- Saved CNN training history\n",
            "-- Saved LSTM training history\n",
            "\n",
            "--  DETAILED PERFORMANCE REPORT\n",
            "==================================================\n",
            "\n",
            "ðŸ† MODEL RANKINGS:\n",
            "   1D CNN               1.0000 â­â­â­â­â­\n",
            "   LSTM                 0.3938 â­â­â­\n",
            "-- Saved detailed reports to results/ folder\n",
            "-- Saving deep learning results...\n",
            "\n",
            "--  DETAILED PERFORMANCE REPORT\n",
            "==================================================\n",
            "\n",
            "ðŸ† MODEL RANKINGS:\n",
            "   1D CNN               1.0000 â­â­â­â­â­\n",
            "   LSTM                 0.3938 â­â­â­\n",
            "-- Saved detailed reports to results/ folder\n",
            "--  Deep learning training completed successfully!\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "--  STEP 4: RESULTS ANALYSIS AND SUMMARY\n",
            "__________________________________________________\n",
            "ðŸ“ˆ Generating comprehensive results summary...\n",
            "--  Loaded traditional ML results\n",
            "--  Loaded deep learning results\n",
            "\n",
            "--  COMPREHENSIVE PERFORMANCE REPORT\n",
            "==================================================\n",
            "\n",
            "ðŸ† MODEL RANKINGS:\n",
            "----------------------------------------\n",
            "   ðŸ¥‡ RandomForest         100.00% â­â­â­â­â­\n",
            "   ðŸ¥ˆ 1D CNN               100.00% â­â­â­â­â­\n",
            "   ðŸ¥‰ LogisticRegression    93.75% â­â­â­â­\n",
            "    4 SVM                   88.75% â­â­â­â­\n",
            "    5 LSTM                  39.38% â­\n",
            "\n",
            "--  BEST PERFORMING MODEL:\n",
            "   Model:    RandomForest\n",
            "   Accuracy: 1.0000 (100.00%)\n",
            "   Type:     Traditional ML\n",
            "\n",
            "ðŸ“ˆ PERFORMANCE STATISTICS:\n",
            "   Total Models:    5\n",
            "   Average Accuracy: 0.8438\n",
            "   Best Accuracy:    1.0000\n",
            "   Worst Accuracy:   0.3938\n",
            "\n",
            "--  MODEL TYPE BREAKDOWN:\n",
            "   Traditional ML        3 models, avg: 0.9417\n",
            "   Deep Learning         2 models, avg: 0.6969\n",
            "--  Results summary completed successfully!\n",
            "\n",
            "======================================================================\n",
            "ðŸŽ‰ PIPELINE EXECUTION COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "--  PIPELINE STATUS:\n",
            "   --  Data Loading\n",
            "   --  Traditional Ml\n",
            "   --  Deep Learning\n",
            "   --  Visualization\n",
            "\n",
            "--  DATA SUMMARY:\n",
            "   Samples: 794\n",
            "   Classes: 6\n",
            "   Signal Length: 1024\n",
            "\n",
            "â±ï¸  EXECUTION TIME:\n",
            "   Total: 2 minutes 54 seconds\n",
            "\n",
            "-- OUTPUTS GENERATED:\n",
            "   results: 13 files\n",
            "   models: 7 files\n",
            "   data/processed: 3 files\n",
            "\n",
            "======================================================================\n",
            "--  Pipeline completed with acceptable success!\n",
            "\n",
            "--  Pipeline executed successfully!\n",
            "--  Generating plots from results/ ...\n",
            "   -- Loaded probabilities for LogisticRegression from /Users/blongho/dev/eel5825-project/results/LogisticRegression_proba.csv\n",
            "   -- Loaded probabilities for RandomForest from /Users/blongho/dev/eel5825-project/results/RandomForest_proba.csv\n",
            "   -- Loaded probabilities for SVM from /Users/blongho/dev/eel5825-project/results/SVM_proba.csv\n",
            "   -- Loaded probabilities for 1D CNN from /Users/blongho/dev/eel5825-project/results/1D_CNN_proba.csv\n",
            "   -- Loaded probabilities for LSTM from /Users/blongho/dev/eel5825-project/results/LSTM_proba.csv\n",
            "-- Saved metrics table (Accuracy/F1/ROC-AUC) to /Users/blongho/dev/eel5825-project/results/model_metrics.csv\n",
            "--  Saved model accuracy bar chart to /Users/blongho/dev/eel5825-project/results/figures/model_accuracy_comparison.png\n",
            "-- Saved CNN accuracy history to /Users/blongho/dev/eel5825-project/results/figures/CNN_accuracy_history.png\n",
            "-- Saved CNN loss history to /Users/blongho/dev/eel5825-project/results/figures/CNN_loss_history.png\n",
            "-- Saved LSTM accuracy history to /Users/blongho/dev/eel5825-project/results/figures/LSTM_accuracy_history.png\n",
            "-- Saved LSTM loss history to /Users/blongho/dev/eel5825-project/results/figures/LSTM_loss_history.png\n",
            "-- Saved confusion matrix for LogisticRegression to /Users/blongho/dev/eel5825-project/results/figures/confusion_matrix_LogisticRegression.png\n",
            "-- Saved confusion matrix for RandomForest to /Users/blongho/dev/eel5825-project/results/figures/confusion_matrix_RandomForest.png\n",
            "-- Saved confusion matrix for SVM to /Users/blongho/dev/eel5825-project/results/figures/confusion_matrix_SVM.png\n",
            "-- Saved confusion matrix for 1D CNN to /Users/blongho/dev/eel5825-project/results/figures/confusion_matrix_1D CNN.png\n",
            "-- Saved confusion matrix for LSTM to /Users/blongho/dev/eel5825-project/results/figures/confusion_matrix_LSTM.png\n",
            "   -- Loaded probabilities for RandomForest from /Users/blongho/dev/eel5825-project/results/RandomForest_proba.csv\n",
            "-- Saved ROC curve for RandomForest to /Users/blongho/dev/eel5825-project/results/figures/roc_curve_RandomForest.png\n",
            "   -- Loaded probabilities for 1D CNN from /Users/blongho/dev/eel5825-project/results/1D_CNN_proba.csv\n",
            "-- Saved ROC curve for 1D CNN to /Users/blongho/dev/eel5825-project/results/figures/roc_curve_1D_CNN.png\n",
            "--  Plot generation completed.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import traceback\n",
        "from plot_results import main\n",
        "\n",
        "try:\n",
        "  pipeline = PipelineExecutor()\n",
        "  success = pipeline.run_pipeline()\n",
        "  if success:\n",
        "      print(\"\\n--  Pipeline executed successfully!\")\n",
        "      main()  # Call visualization only if pipeline succeeded\n",
        "  else:\n",
        "      print(\"\\n--  Pipeline executed with some issues. Check logs above.\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\\n--  Pipeline interrupted by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--  Unexpected pipeline failure: {e}\")\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
