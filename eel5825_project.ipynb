{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EEL5825 - Machine Learning and Pattern Recognition \n",
        "## Course Project\n",
        "\n",
        "Student: Longho Bernard Che\n",
        "\n",
        "Student ID: 5756998"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25RbJ_qbeabL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmStZgh6g9Zy"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, config_path=\"config/parameters.yaml\", use_cached=True):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        config_path : str\n",
        "            Relative path to the YAML config from project root.\n",
        "        use_cached : bool\n",
        "            If True, will try to reuse data/processed/signals.npy etc.\n",
        "            before reprocessing raw .mat files.\n",
        "        \"\"\"\n",
        "        # Get the absolute path to config file\n",
        "        current_dir = os.getcwd()\n",
        "        project_root = current_dir\n",
        "        config_path = os.path.join(project_root, config_path)\n",
        "\n",
        "        with open(config_path, \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        self.files = self.config[\"dataset\"][\"files\"]\n",
        "        self.segment_length = self.config[\"preprocessing\"][\"segment_length\"]\n",
        "        self.samples_per_class = self.config[\"preprocessing\"][\"samples_per_class\"]\n",
        "        self.project_root = project_root\n",
        "        self.random_state = self.config[\"preprocessing\"][\"random_state\"]\n",
        "        self.use_cached = use_cached\n",
        "\n",
        "    def _load_cached_data(self):\n",
        "        \"\"\"Try to load cached processed data if available.\"\"\"\n",
        "        processed_dir = os.path.join(self.project_root, \"data\", \"processed\")\n",
        "        signals_path = os.path.join(processed_dir, \"signals.npy\")\n",
        "        labels_path = os.path.join(processed_dir, \"labels.npy\")\n",
        "        label_map_path = os.path.join(processed_dir, \"label_map.json\")\n",
        "\n",
        "        if (\n",
        "            os.path.exists(signals_path)\n",
        "            and os.path.exists(labels_path)\n",
        "            and os.path.exists(label_map_path)\n",
        "        ):\n",
        "            print(\"--  Reusing cached processed data from data/processed/ ...\")\n",
        "            X = np.load(signals_path)\n",
        "            y = np.load(labels_path)\n",
        "            with open(label_map_path, \"r\") as f:\n",
        "                label_map_raw = json.load(f)\n",
        "\n",
        "            # JSON turns int keys into strings â†’ convert back\n",
        "            label_map = {int(k): v for k, v in label_map_raw.items()}\n",
        "\n",
        "            print(f\"   Loaded {X.shape[0]} samples from cache\")\n",
        "            print(f\"   Classes: {list(label_map.values())}\")\n",
        "            return X, y, label_map\n",
        "\n",
        "        return None, None, None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and segment all vibration data.\"\"\"\n",
        "        # Try cached version first\n",
        "        if self.use_cached:\n",
        "            X_cached, y_cached, label_map_cached = self._load_cached_data()\n",
        "            if X_cached is not None:\n",
        "                return X_cached, y_cached, label_map_cached\n",
        "\n",
        "        all_signals = []\n",
        "        all_labels = []\n",
        "        label_map = {}\n",
        "\n",
        "        print(\"--  Loading CWRU Bearing Dataset from raw .mat files...\")\n",
        "\n",
        "        for label, (class_name, filename) in enumerate(self.files.items()):\n",
        "            print(f\"   Loading {class_name} from {filename}...\")\n",
        "            filepath = os.path.join(self.project_root, \"data\", \"raw\", filename)\n",
        "\n",
        "            try:\n",
        "                mat_data = loadmat(filepath)\n",
        "\n",
        "                # Extract vibration signal (handle different key names)\n",
        "                signal_keys = [\n",
        "                    key\n",
        "                    for key in mat_data.keys()\n",
        "                    if \"DE_time\" in key or \"X\" in key or \"driven\" in key\n",
        "                ]\n",
        "                if not signal_keys:\n",
        "                    print(f\"     --  No vibration signal found in {filename}\")\n",
        "                    continue\n",
        "\n",
        "                signal_key = signal_keys[0]\n",
        "                signal = mat_data[signal_key].flatten()\n",
        "\n",
        "                # Segment into fixed-length samples\n",
        "                segments = []\n",
        "                for i in range(\n",
        "                    0, len(signal) - self.segment_length, self.segment_length\n",
        "                ):\n",
        "                    segments.append(signal[i : i + self.segment_length])\n",
        "                    if len(segments) >= self.samples_per_class:\n",
        "                        break\n",
        "\n",
        "                all_signals.extend(segments)\n",
        "                all_labels.extend([label] * len(segments))\n",
        "                label_map[label] = class_name\n",
        "                print(f\"     --  Loaded {len(segments)} samples\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"     --  ERROR loading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = np.array(all_signals)\n",
        "        y = np.array(all_labels)\n",
        "\n",
        "        # Save processed data\n",
        "        self.save_processed_data(X, y, label_map)\n",
        "\n",
        "        print(f\"\\n--  Dataset Summary:\")\n",
        "        print(f\"   Total samples: {len(X)}\")\n",
        "        print(f\"   Number of classes: {len(np.unique(y))}\")\n",
        "        print(f\"   Signal length: {self.segment_length}\")\n",
        "        print(f\"   Classes: {list(label_map.values())}\")\n",
        "\n",
        "        return X, y, label_map\n",
        "\n",
        "    def create_features(self, signals):\n",
        "        \"\"\"Extract features for traditional ML models.\"\"\"\n",
        "        print(\"--  Extracting features for traditional ML...\")\n",
        "\n",
        "        features = []\n",
        "        for signal in signals:\n",
        "            # Time-domain features\n",
        "            mean = np.mean(signal)\n",
        "            std = np.std(signal)\n",
        "            rms = np.sqrt(np.mean(signal**2))\n",
        "            peak_to_peak = np.max(signal) - np.min(signal)\n",
        "            skewness = np.mean((signal - mean) ** 3) / (std**3) if std != 0 else 0\n",
        "            kurtosis = np.mean((signal - mean) ** 4) / (std**4) if std != 0 else 0\n",
        "\n",
        "            feature_vector = [mean, std, rms, peak_to_peak, skewness, kurtosis]\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        feature_names = [\"mean\", \"std\", \"rms\", \"peak_to_peak\", \"skewness\", \"kurtosis\"]\n",
        "        features_array = np.array(features)\n",
        "\n",
        "        return features_array, feature_names\n",
        "\n",
        "    def save_processed_data(self, X, y, label_map):\n",
        "        \"\"\"Save processed data to files.\"\"\"\n",
        "        processed_dir = os.path.join(self.project_root, \"data\", \"processed\")\n",
        "        os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "        np.save(os.path.join(processed_dir, \"signals.npy\"), X)\n",
        "        np.save(os.path.join(processed_dir, \"labels.npy\"), y)\n",
        "\n",
        "        with open(os.path.join(processed_dir, \"label_map.json\"), \"w\") as f:\n",
        "            json.dump(label_map, f, indent=2)\n",
        "\n",
        "        print(\"-- Saved processed data to data/processed/\")\n",
        "\n",
        "    def prepare_splits(self, X, y, X_features=None):\n",
        "        \"\"\"\n",
        "        Prepare train/validation/test splits with CONSISTENT indices\n",
        "        between raw signals and feature matrices.\n",
        "\n",
        "        Uses preprocessing.test_size and preprocessing.val_size from config\n",
        "        as FINAL fractions of the whole dataset.\n",
        "        \"\"\"\n",
        "        random_state = self.random_state\n",
        "        test_size_cfg = float(self.config[\"preprocessing\"][\"test_size\"])\n",
        "        val_size_cfg = float(self.config[\"preprocessing\"][\"val_size\"])\n",
        "\n",
        "        print(\n",
        "            f\"\\n--  Preparing data splits (test={test_size_cfg}, val={val_size_cfg})...\"\n",
        "        )\n",
        "\n",
        "        if test_size_cfg + val_size_cfg >= 1.0:\n",
        "            raise ValueError(\n",
        "                \"test_size + val_size must be < 1.0 in config/preprocessing.\"\n",
        "            )\n",
        "\n",
        "        # First: split into train vs holdout (val+test)\n",
        "        holdout_size = test_size_cfg + val_size_cfg\n",
        "        relative_test_size = test_size_cfg / holdout_size  # within holdout\n",
        "\n",
        "        indices = np.arange(len(y))\n",
        "\n",
        "        # Split indices â€“ ensures consistent splits for both X and X_features\n",
        "        idx_train, idx_hold, y_train, y_hold = train_test_split(\n",
        "            indices,\n",
        "            y,\n",
        "            test_size=holdout_size,\n",
        "            random_state=random_state,\n",
        "            stratify=y,\n",
        "        )\n",
        "\n",
        "        idx_val, idx_test, y_val, y_test = train_test_split(\n",
        "            idx_hold,\n",
        "            y_hold,\n",
        "            test_size=relative_test_size,\n",
        "            random_state=random_state,\n",
        "            stratify=y_hold,\n",
        "        )\n",
        "\n",
        "        # Raw signals for deep learning\n",
        "        X_train_raw = X[idx_train]\n",
        "        X_val_raw = X[idx_val]\n",
        "        X_test_raw = X[idx_test]\n",
        "\n",
        "        # Features for traditional ML (if provided)\n",
        "        if X_features is not None:\n",
        "            X_train_feat = X_features[idx_train]\n",
        "            X_val_feat = X_features[idx_val]\n",
        "            X_test_feat = X_features[idx_test]\n",
        "\n",
        "            print(\n",
        "                f\"   Traditional ML: {X_train_feat.shape[0]} train, \"\n",
        "                f\"{X_val_feat.shape[0]} val, {X_test_feat.shape[0]} test\"\n",
        "            )\n",
        "        else:\n",
        "            X_train_feat = X_val_feat = X_test_feat = None\n",
        "\n",
        "        # Reshape for deep learning (N, L, 1)\n",
        "        X_train_raw = X_train_raw.reshape(X_train_raw.shape[0], X_train_raw.shape[1], 1)\n",
        "        X_val_raw = X_val_raw.reshape(X_val_raw.shape[0], X_val_raw.shape[1], 1)\n",
        "        X_test_raw = X_test_raw.reshape(X_test_raw.shape[0], X_test_raw.shape[1], 1)\n",
        "\n",
        "        print(\n",
        "            f\"   Deep Learning: {X_train_raw.shape[0]} train, \"\n",
        "            f\"{X_val_raw.shape[0]} val, {X_test_raw.shape[0]} test\"\n",
        "        )\n",
        "\n",
        "        splits = {\n",
        "            \"traditional\": {\n",
        "                \"X_train\": X_train_feat,\n",
        "                \"X_val\": X_val_feat,\n",
        "                \"X_test\": X_test_feat,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_val\": y_val,\n",
        "                \"y_test\": y_test,\n",
        "            },\n",
        "            \"deep_learning\": {\n",
        "                \"X_train\": X_train_raw,\n",
        "                \"X_val\": X_val_raw,\n",
        "                \"X_test\": X_test_raw,\n",
        "                \"y_train\": y_train,\n",
        "                \"y_val\": y_val,\n",
        "                \"y_test\": y_test,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        return splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-zkWg-4hJr_"
      },
      "source": [
        "## Traditional Machine Learning Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NMhLpcshNpI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import yaml\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbogzKEyhQm4"
      },
      "outputs": [],
      "source": [
        "class TraditionalML:\n",
        "    def __init__(self):\n",
        "        # Get project root\n",
        "        current_dir = os.getcwd()\n",
        "        self.project_root = current_dir\n",
        "\n",
        "        config_path = os.path.join(self.project_root, \"config\", \"parameters.yaml\")\n",
        "        with open(config_path, \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        self.models = {\n",
        "            \"LogisticRegression\": LogisticRegression(\n",
        "                max_iter=self.config[\"models\"][\"traditional\"][\"logistic_regression\"][\n",
        "                    \"max_iter\"\n",
        "                ],\n",
        "                random_state=42,\n",
        "            ),\n",
        "            \"RandomForest\": RandomForestClassifier(\n",
        "                n_estimators=self.config[\"models\"][\"traditional\"][\"random_forest\"][\n",
        "                    \"n_estimators\"\n",
        "                ],\n",
        "                random_state=42,\n",
        "            ),\n",
        "            \"SVM\": SVC(\n",
        "                kernel=self.config[\"models\"][\"traditional\"][\"svm\"][\"kernel\"],\n",
        "                random_state=42,\n",
        "                probability=True,\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "    def train_and_evaluate(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Train and evaluate all traditional ML models \"\"\"\n",
        "        print(\"\\n-- Training Traditional ML Models...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Check data shapes\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG - X_train shape: {X_train.shape}\")\n",
        "        print(f\"DEBUG - X_test shape: {X_test.shape}\")\n",
        "        print(f\"DEBUG - y_train shape: {y_train.shape}\")\n",
        "        print(f\"DEBUG - y_test shape: {y_test.shape}\")\n",
        "        \"\"\"\n",
        "\n",
        "        if X_train is None or X_test is None:\n",
        "            print(\"-- ERROR: Feature data is None. Check data loading.\")\n",
        "            return {}\n",
        "\n",
        "        # Hyperparameter tuning for RandomForest\n",
        "        try:\n",
        "            self.tune_random_forest(X_train, y_train)\n",
        "        except Exception as e:\n",
        "            print(f\"-- RandomForest tuning failed, using default params: {e}\")\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "\n",
        "            try:\n",
        "                # Train model\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Predictions (hard labels)\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "                # Check prediction shape\n",
        "                #print(f\"DEBUG - y_pred shape: {y_pred.shape}\")\n",
        "                #print(f\"DEBUG - y_test shape: {y_test.shape}\")\n",
        "\n",
        "                # Try to get class probabilities for ROC/AUC\n",
        "                y_proba = None\n",
        "                if hasattr(model, \"predict_proba\"):\n",
        "                    try:\n",
        "                        y_proba = model.predict_proba(X_test)\n",
        "                        print(f\"DEBUG - y_proba shape: {y_proba.shape}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"   -- Could not compute predict_proba for {name}: {e}\")\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "                # Store results\n",
        "                self.results[name] = {\n",
        "                    \"model\": model,\n",
        "                    \"accuracy\": accuracy,\n",
        "                    \"predictions\": y_pred,\n",
        "                    \"true_labels\": y_test,\n",
        "                }\n",
        "\n",
        "                print(f\"   -- {name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "                # Save model\n",
        "                models_dir = os.path.join(self.project_root, \"models\")\n",
        "                os.makedirs(models_dir, exist_ok=True)\n",
        "                joblib.dump(model, os.path.join(models_dir, f\"{name.lower()}.pkl\"))\n",
        "\n",
        "                # Save probabilities for ROC if available\n",
        "                if y_proba is not None:\n",
        "                    # Use sorted unique labels to define class order\n",
        "                    class_labels = sorted(np.unique(y_train))\n",
        "                    proba_df = pd.DataFrame(\n",
        "                        y_proba,\n",
        "                        columns=[f\"class_{c}\" for c in class_labels],\n",
        "                    )\n",
        "                    proba_df[\"true_label\"] = y_test\n",
        "\n",
        "                    results_dir = os.path.join(self.project_root, \"results\")\n",
        "                    os.makedirs(results_dir, exist_ok=True)\n",
        "                    proba_path = os.path.join(results_dir, f\"{name}_proba.csv\")\n",
        "                    proba_df.to_csv(proba_path, index=False)\n",
        "                    print(f\"   -- Saved probabilities for {name} to {proba_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   -- ERROR training {name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def tune_random_forest(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Simple hyperparameter tuning for RandomForest using GridSearchCV.\n",
        "\n",
        "        Searches over a small grid and updates self.models['RandomForest']\n",
        "        to the best estimator found (based on CV accuracy).\n",
        "        \"\"\"\n",
        "        if \"RandomForest\" not in self.models:\n",
        "            print(\"-- RandomForest not found in models dict; skipping RF tuning.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n-- Hyperparameter tuning for RandomForest (3-fold CV)...\")\n",
        "\n",
        "        rf = self.models[\"RandomForest\"]\n",
        "\n",
        "        param_grid = {\n",
        "            \"n_estimators\": [50, 100, 200],\n",
        "            \"max_depth\": [None, 10, 20],\n",
        "            \"min_samples_split\": [2, 5],\n",
        "            \"min_samples_leaf\": [1, 2],\n",
        "            \"max_features\": [\"sqrt\", \"log2\"],\n",
        "        }\n",
        "\n",
        "        grid = GridSearchCV(\n",
        "            rf,\n",
        "            param_grid=param_grid,\n",
        "            cv=3,\n",
        "            scoring=\"accuracy\",\n",
        "            n_jobs=-1,\n",
        "            verbose=1,\n",
        "        )\n",
        "\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"   -- Best RF params: {grid.best_params_}\")\n",
        "        print(f\"   -- Best CV accuracy: {grid.best_score_:.4f}\")\n",
        "\n",
        "        # Replace the RandomForest model with the best estimator\n",
        "        self.models[\"RandomForest\"] = grid.best_estimator_\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save traditional ML results.\"\"\"\n",
        "        results_dir = os.path.join(self.project_root, \"results\")\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        # Save accuracy comparison\n",
        "        results_df = pd.DataFrame(\n",
        "            {\n",
        "                \"Model\": list(self.results.keys()),\n",
        "                \"Accuracy\": [result[\"accuracy\"] for result in self.results.values()],\n",
        "                \"Type\": \"Traditional ML\",\n",
        "            }\n",
        "        )\n",
        "        results_df.to_csv(\n",
        "            os.path.join(results_dir, \"traditional_ml_results.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        # Save detailed predictions\n",
        "        all_predictions = []\n",
        "        for model_name, result in self.results.items():\n",
        "            for i, (true, pred) in enumerate(\n",
        "                zip(result[\"true_labels\"], result[\"predictions\"])\n",
        "            ):\n",
        "                all_predictions.append(\n",
        "                    {\n",
        "                        \"model\": model_name,\n",
        "                        \"true_label\": true,\n",
        "                        \"predicted_label\": pred,\n",
        "                        \"correct\": true == pred,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        pd.DataFrame(all_predictions).to_csv(\n",
        "            os.path.join(results_dir, \"traditional_predictions.csv\"), index=False\n",
        "        )\n",
        "        print(\"-- Saved traditional ML results to results/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8KxM03BhXJg"
      },
      "source": [
        "## Deep Learning Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_lqlUGghViy",
        "outputId": "7bdf4383-c18f-4893-bf08-f044e8043f85"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Optional plotting imports (kept in case you want to extend later)\n",
        "import matplotlib.pyplot as plt  # noqa: F401\n",
        "import seaborn as sns  # noqa: F401\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras import layers, models, callbacks\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "    TF_AVAILABLE = True\n",
        "    print(\"--  Using TensorFlow Keras\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        import keras\n",
        "        from keras import layers, models, callbacks\n",
        "        from keras.optimizers import Adam\n",
        "\n",
        "        TF_AVAILABLE = True\n",
        "        print(\"--  Using standalone Keras\")\n",
        "    except ImportError:\n",
        "        TF_AVAILABLE = False\n",
        "        print(\"--  Deep learning libraries not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPrEIVjEhhAo"
      },
      "outputs": [],
      "source": [
        "class DeepLearningModels:\n",
        "    def __init__(self):\n",
        "        current_dir = os.getcwd()\n",
        "        self.project_root = current_dir\n",
        "\n",
        "        config_path = os.path.join(self.project_root, \"config\", \"parameters.yaml\")\n",
        "        with open(config_path, \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "        # Deep learning hyperparameters from config\n",
        "        dl_cfg = self.config.get(\"models\", {}).get(\"deep_learning\", {})\n",
        "        self.epochs = int(dl_cfg.get(\"epochs\", 30))\n",
        "        self.batch_size = int(dl_cfg.get(\"batch_size\", 32))\n",
        "        self.learning_rate = float(dl_cfg.get(\"learning_rate\", 0.001))\n",
        "\n",
        "        # Ensure models and results directories exist\n",
        "        os.makedirs(os.path.join(self.project_root, \"models\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.project_root, \"results\"), exist_ok=True)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # DATA PREPROCESSING\n",
        "    # ------------------------------------------------------------------\n",
        "    def preprocess_data(self, X_train, X_val, X_test):\n",
        "        \"\"\"\n",
        "        Preprocess data for deep learning models.\n",
        "\n",
        "        Uses GLOBAL z-score normalization based on the TRAIN set only.\n",
        "        This preserves relative amplitude differences between samples,\n",
        "        which is important for vibration/fault diagnosis.\n",
        "        \"\"\"\n",
        "        print(\"--  Preprocessing data for deep learning...\")\n",
        "\n",
        "        # Ensure float32 and correct shape\n",
        "        X_train = X_train.astype(np.float32)\n",
        "        X_val = X_val.astype(np.float32)\n",
        "        X_test = X_test.astype(np.float32)\n",
        "\n",
        "        # Compute global mean/std on TRAIN ONLY\n",
        "        train_mean = X_train.mean()\n",
        "        train_std = X_train.std()\n",
        "\n",
        "        if train_std < 1e-8:\n",
        "            print(\"âš ï¸ Train std is extremely small; skipping normalization.\")\n",
        "            return X_train, X_val, X_test\n",
        "\n",
        "        print(f\"   Global train mean: {train_mean:.5f}, std: {train_std:.5f}\")\n",
        "\n",
        "        X_train_norm = (X_train - train_mean) / train_std\n",
        "        X_val_norm = (X_val - train_mean) / train_std\n",
        "        X_test_norm = (X_test - train_mean) / train_std\n",
        "\n",
        "        print(\n",
        "            f\"   Data shapes - Train: {X_train_norm.shape}, \"\n",
        "            f\"Val: {X_val_norm.shape}, Test: {X_test_norm.shape}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"   Train range - Min: {X_train_norm.min():.3f}, \"\n",
        "            f\"Max: {X_train_norm.max():.3f}\"\n",
        "        )\n",
        "\n",
        "        return X_train_norm, X_val_norm, X_test_norm\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_1d_cnn(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Create a 1D CNN model for vibration signals.\n",
        "\n",
        "        This is the '1D CNN' mentioned in your project proposal.\n",
        "        \"\"\"\n",
        "        print(\"   Building 1D CNN architecture...\")\n",
        "\n",
        "        model = models.Sequential(\n",
        "            [\n",
        "                layers.Input(shape=input_shape, name=\"input_layer\"),\n",
        "                # Convolutional block 1\n",
        "                layers.Conv1D(16, kernel_size=7, activation=\"relu\", padding=\"same\"),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.MaxPooling1D(2),\n",
        "                # REMOVE Dropout(0.2) here\n",
        "\n",
        "                # Convolutional block 2\n",
        "                layers.Conv1D(32, kernel_size=5, activation=\"relu\", padding=\"same\"),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.MaxPooling1D(2),\n",
        "                # REMOVE Dropout(0.2) here\n",
        "\n",
        "                # Convolutional block 3\n",
        "                layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
        "                layers.BatchNormalization(),\n",
        "                layers.GlobalAveragePooling1D(),\n",
        "\n",
        "                # Dense head\n",
        "                layers.Dense(64, activation=\"relu\"),\n",
        "                layers.Dropout(0.3),  # keep this one\n",
        "                layers.Dense(num_classes, activation=\"softmax\"),\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"sparse_categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_lstm(self, input_shape, num_classes):\n",
        "        \"\"\"\n",
        "        Create an LSTM model for temporal vibration patterns.\n",
        "\n",
        "        Simplified vs. your previous version to better suit the\n",
        "        relatively small dataset and reduce over-regularization.\n",
        "        \"\"\"\n",
        "        print(\"   Building LSTM architecture...\")\n",
        "\n",
        "        model = models.Sequential(\n",
        "            [\n",
        "                layers.Input(shape=input_shape, name=\"input_layer\"),\n",
        "                layers.LSTM(\n",
        "                    64,\n",
        "                    return_sequences=False,\n",
        "                    dropout=0.0,\n",
        "                    recurrent_dropout=0.0,\n",
        "                    name=\"lstm_1\",\n",
        "                ),\n",
        "                layers.Dense(32, activation=\"relu\", name=\"dense_1\"),\n",
        "                layers.Dropout(0.2, name=\"dropout_1\"),\n",
        "                layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        optimizer = Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"sparse_categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # TRAINING UTILITIES\n",
        "    # ------------------------------------------------------------------\n",
        "    def get_callbacks(self, model_name):\n",
        "        \"\"\"Get training callbacks for better convergence.\"\"\"\n",
        "        models_dir = os.path.join(self.project_root, \"models\")\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        callbacks_list = [\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor=\"val_accuracy\",\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_accuracy\",\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                filepath=os.path.join(models_dir, f\"best_{model_name}.h5\"),\n",
        "                monitor=\"val_accuracy\",\n",
        "                save_best_only=True,\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        return callbacks_list\n",
        "\n",
        "    def analyze_dataset(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "        \"\"\"Analyze dataset characteristics.\"\"\"\n",
        "        print(\"\\n--  DATASET ANALYSIS:\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        print(f\"   Training samples: {X_train.shape[0]}\")\n",
        "        print(f\"   Validation samples: {X_val.shape[0]}\")\n",
        "        print(f\"   Test samples: {X_test.shape[0]}\")\n",
        "        print(f\"   Input shape: {X_train.shape[1:]}\")\n",
        "        print(f\"   Number of classes: {len(np.unique(y_train))}\")\n",
        "\n",
        "        # Class distribution\n",
        "        train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
        "        val_unique, val_counts = np.unique(y_val, return_counts=True)\n",
        "        test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
        "\n",
        "        print(f\"\\n   Class Distribution:\")\n",
        "        print(f\"     Train: {dict(zip(train_unique, train_counts))}\")\n",
        "        print(f\"     Val:   {dict(zip(val_unique, val_counts))}\")\n",
        "        print(f\"     Test:  {dict(zip(test_unique, test_counts))}\")\n",
        "\n",
        "        # Data statistics\n",
        "        print(f\"\\n   Data Statistics:\")\n",
        "        print(f\"     Train - Min: {X_train.min():.3f}, Max: {X_train.max():.3f}\")\n",
        "        print(f\"     Val   - Min: {X_val.min():.3f}, Max: {X_val.max():.3f}\")\n",
        "        print(f\"     Test  - Min: {X_test.min():.3f}, Max: {X_test.max():.3f}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def train_single_model(self, model, model_name, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train a single model with comprehensive logging.\"\"\"\n",
        "        print(f\"\\n--  Training {model_name}...\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Display model architecture\n",
        "        print(f\"   {model_name} Architecture:\")\n",
        "        model.summary()\n",
        "\n",
        "        # Get callbacks\n",
        "        training_callbacks = self.get_callbacks(model_name)\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=self.epochs,  # from config\n",
        "            batch_size=self.batch_size,  # from config\n",
        "            callbacks=training_callbacks,\n",
        "            verbose=1,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate_model(self, model, model_name, X_test, y_test):\n",
        "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "        print(f\"\\n-- Evaluating {model_name}...\")\n",
        "\n",
        "        # Basic evaluation\n",
        "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Predictions (probabilities and hard labels)\n",
        "        y_pred_proba = model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "        # Additional metrics\n",
        "        test_accuracy_manual = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"   Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"   Test Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"   Manual Accuracy: {test_accuracy_manual:.4f}\")\n",
        "\n",
        "        # Save probabilities for ROC/AUC\n",
        "        try:\n",
        "            results_dir = os.path.join(self.project_root, \"results\")\n",
        "            os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "            # Map internal name \"CNN\" to display name \"1D CNN\" for consistency\n",
        "            display_name = \"1D CNN\" if model_name == \"CNN\" else model_name\n",
        "\n",
        "            # Use a file-friendly stem\n",
        "            stem = display_name.replace(\" \", \"_\")\n",
        "            proba_path = os.path.join(results_dir, f\"{stem}_proba.csv\")\n",
        "\n",
        "            class_labels = sorted(np.unique(y_test))\n",
        "            proba_df = pd.DataFrame(\n",
        "                y_pred_proba,\n",
        "                columns=[f\"class_{c}\" for c in class_labels],\n",
        "            )\n",
        "            proba_df[\"true_label\"] = y_test\n",
        "            proba_df.to_csv(proba_path, index=False)\n",
        "            print(f\"   -- Saved probabilities for {display_name} to {proba_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   -- Could not save probabilities for {model_name}: {e}\")\n",
        "\n",
        "        return {\n",
        "            \"model\": model,\n",
        "            \"accuracy\": test_accuracy,\n",
        "            \"predictions\": y_pred,\n",
        "            \"probabilities\": y_pred_proba,\n",
        "            \"true_labels\": y_test,\n",
        "            \"loss\": test_loss,\n",
        "        }\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # MAIN TRAINING ENTRY POINT\n",
        "    # ------------------------------------------------------------------\n",
        "    def train_models(self, X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "        \"\"\"\n",
        "        Main training function for deep learning models.\n",
        "\n",
        "        Trains exactly the models required by your proposal:\n",
        "        - 1D CNN\n",
        "        - LSTM\n",
        "        \"\"\"\n",
        "        if not TF_AVAILABLE:\n",
        "            print(\"--  Deep learning libraries not available. Skipping DL training.\")\n",
        "            return {}\n",
        "\n",
        "        print(\"\\n DEEP LEARNING MODEL TRAINING\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Preprocess data\n",
        "        X_train_processed, X_val_processed, X_test_processed = self.preprocess_data(\n",
        "            X_train, X_val, X_test\n",
        "        )\n",
        "\n",
        "        # Analyze dataset\n",
        "        self.analyze_dataset(\n",
        "            X_train_processed,\n",
        "            y_train,\n",
        "            X_val_processed,\n",
        "            y_val,\n",
        "            X_test_processed,\n",
        "            y_test,\n",
        "        )\n",
        "\n",
        "        num_classes = len(np.unique(y_train))\n",
        "        input_shape = (X_train_processed.shape[1], X_train_processed.shape[2])\n",
        "\n",
        "        # Define models to train (internal keys are short; display names handled later)\n",
        "        models_to_train = {\n",
        "            \"CNN\": self.create_1d_cnn(input_shape, num_classes),\n",
        "            \"LSTM\": self.create_lstm(input_shape, num_classes),\n",
        "        }\n",
        "\n",
        "        trained_models = {}\n",
        "        training_histories = {}\n",
        "\n",
        "\n",
        "        for model_name, model in models_to_train.items():\n",
        "            try:\n",
        "                # Train model\n",
        "                history = self.train_single_model(\n",
        "                    model,\n",
        "                    model_name,\n",
        "                    X_train_processed,\n",
        "                    y_train,\n",
        "                    X_val_processed,\n",
        "                    y_val,\n",
        "                )\n",
        "\n",
        "                # Evaluate model\n",
        "                results = self.evaluate_model(\n",
        "                    model, model_name, X_test_processed, y_test\n",
        "                )\n",
        "\n",
        "                # Store results\n",
        "                trained_models[model_name] = results\n",
        "                training_histories[model_name] = history.history\n",
        "\n",
        "                # Save model\n",
        "                model_path = os.path.join(\n",
        "                    self.project_root, \"models\", f\"{model_name}_final.h5\"\n",
        "                )\n",
        "                model.save(model_path)\n",
        "                print(f\"-- Saved {model_name} to {model_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"--  Error training {model_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Store final results\n",
        "        self.results = trained_models\n",
        "\n",
        "        # Save training histories\n",
        "        self.save_training_histories(training_histories)\n",
        "\n",
        "        # Generate comprehensive report\n",
        "        self.generate_detailed_report()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # REPORTING\n",
        "    # ------------------------------------------------------------------\n",
        "    def save_training_histories(self, training_histories):\n",
        "        \"\"\"Save training histories for analysis.\"\"\"\n",
        "        results_dir = os.path.join(self.project_root, \"results\")\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "        for model_name, history in training_histories.items():\n",
        "            history_df = pd.DataFrame(history)\n",
        "            history_path = os.path.join(\n",
        "                results_dir, f\"{model_name}_training_history.csv\"\n",
        "            )\n",
        "            history_df.to_csv(history_path, index=False)\n",
        "            print(f\"-- Saved {model_name} training history\")\n",
        "\n",
        "    def generate_detailed_report(self):\n",
        "        \"\"\"Generate detailed performance report.\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"--  No results to generate report\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n--  DETAILED PERFORMANCE REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Create results dataframe\n",
        "        results_data = []\n",
        "        for model_name, result in self.results.items():\n",
        "            # Map internal name \"CNN\" to display name \"1D CNN\" for your report\n",
        "            display_name = \"1D CNN\" if model_name == \"CNN\" else model_name\n",
        "\n",
        "            results_data.append(\n",
        "                {\n",
        "                    \"Model\": display_name,\n",
        "                    \"Accuracy\": result[\"accuracy\"],\n",
        "                    \"Loss\": result[\"loss\"],\n",
        "                    \"Type\": \"Deep Learning\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "        results_df = pd.DataFrame(results_data)\n",
        "\n",
        "        # Save to file\n",
        "        results_dir = os.path.join(self.project_root, \"results\")\n",
        "        results_df.to_csv(\n",
        "            os.path.join(results_dir, \"deep_learning_results.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        # Print ranking\n",
        "        ranked_results = results_df.sort_values(\"Accuracy\", ascending=False)\n",
        "        print(\"\\nðŸ† MODEL RANKINGS:\")\n",
        "        for _, row in ranked_results.iterrows():\n",
        "            stars = \"â­\" * min(5, int(row[\"Accuracy\"] * 10))\n",
        "            print(f\"   {row['Model']:20} {row['Accuracy']:.4f} {stars}\")\n",
        "\n",
        "        # Save predictions\n",
        "        all_predictions = []\n",
        "        for model_name, result in self.results.items():\n",
        "            display_name = \"1D CNN\" if model_name == \"CNN\" else model_name\n",
        "            for true, pred in zip(result[\"true_labels\"], result[\"predictions\"]):\n",
        "                all_predictions.append(\n",
        "                    {\n",
        "                        \"model\": display_name,\n",
        "                        \"true_label\": int(true),\n",
        "                        \"predicted_label\": int(pred),\n",
        "                        \"correct\": bool(true == pred),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        predictions_df = pd.DataFrame(all_predictions)\n",
        "        predictions_df.to_csv(\n",
        "            os.path.join(results_dir, \"deep_learning_predictions.csv\"), index=False\n",
        "        )\n",
        "\n",
        "        print(\"-- Saved detailed reports to results/ folder\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"\n",
        "        Save deep learning results to CSV files.\n",
        "\n",
        "        This mirrors TraditionalML.save_results() so the pipeline can safely call:\n",
        "            dl_models.save_results()\n",
        "\n",
        "        It uses the in-memory `self.results` (populated by `train_models`) and\n",
        "        reuses `generate_detailed_report()` which already writes:\n",
        "\n",
        "            - results/deep_learning_results.csv\n",
        "            - results/deep_learning_predictions.csv\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            print(\n",
        "                \"âš ï¸ No in-memory deep learning results found. \"\n",
        "                \"If you already trained models in a previous run, the CSV files \"\n",
        "                \"in results/ are already on disk. \"\n",
        "                \"If this is a fresh run, call train_models(...) before save_results().\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        self.generate_detailed_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4loxrGMQiekB"
      },
      "source": [
        "## Run the whole experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdGZ1Ivvii7j"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f71sYKVMh31G"
      },
      "outputs": [],
      "source": [
        "class PipelineExecutor:\n",
        "    \"\"\"Main pipeline executor with comprehensive error handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.results = {}\n",
        "        self.pipeline_status = {\n",
        "            \"data_loading\": False,\n",
        "            \"traditional_ml\": False,\n",
        "            \"deep_learning\": False,\n",
        "            \"visualization\": False,\n",
        "        }\n",
        "\n",
        "    def print_header(self):\n",
        "        \"\"\"Print pipeline header\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"--  BEARING FAULT DIAGNOSIS PIPELINE - ROBUST EXECUTION\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    def print_step_header(self, step_number, step_name):\n",
        "        \"\"\"Print step header\"\"\"\n",
        "        print(f\"\\n{'â”'*50}\")\n",
        "        print(f\"--  STEP {step_number}: {step_name}\")\n",
        "        print(f\"{'_'*50}\")\n",
        "\n",
        "    def validate_environment(self):\n",
        "        \"\"\"Validate that all required components are available\"\"\"\n",
        "        print(\"\\nðŸ” VALIDATING ENVIRONMENT...\")\n",
        "\n",
        "        # Check required directories\n",
        "        required_dirs = [\"data/raw\", \"config\"]\n",
        "        for dir_path in required_dirs:\n",
        "            if not os.path.exists(dir_path):\n",
        "                print(f\"--  Missing directory: {dir_path}\")\n",
        "                return False\n",
        "            print(f\"--  Directory exists: {dir_path}\")\n",
        "\n",
        "        # Check required files\n",
        "        required_files = [\"config/parameters.yaml\"]\n",
        "        for file_path in required_files:\n",
        "            if not os.path.exists(file_path):\n",
        "                print(f\"--  Missing file: {file_path}\")\n",
        "                return False\n",
        "            print(f\"--  File exists: {file_path}\")\n",
        "\n",
        "        # Check for data files\n",
        "        data_files = os.listdir(\"data/raw\")\n",
        "        if not data_files:\n",
        "            print(\"--  No data files found in data/raw/\")\n",
        "            print(\"   Please download CWRU .mat files first\")\n",
        "            return False\n",
        "\n",
        "        print(f\"--  Found {len(data_files)} data files in data/raw/\")\n",
        "        return True\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Step 1: Data loading and preprocessing\"\"\"\n",
        "        self.print_step_header(1, \"DATA LOADING AND PREPROCESSING\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            print(\"--  Initializing data loader...\")\n",
        "            loader = DataLoader(use_cached=True)  # or False to force reprocessing\n",
        "\n",
        "            print(\"--  Loading and segmenting vibration data...\")\n",
        "            X, y, label_map = loader.load_data()\n",
        "\n",
        "            if len(X) == 0:\n",
        "                raise ValueError(\"No data loaded - check your .mat files\")\n",
        "\n",
        "            print(\"--  Extracting features for traditional ML...\")\n",
        "            features, feature_names = loader.create_features(X)\n",
        "\n",
        "            print(\"--  Preparing train/validation/test splits...\")\n",
        "            splits = loader.prepare_splits(X, y, features)\n",
        "\n",
        "            # Store results\n",
        "            self.results[\"data\"] = {\n",
        "                \"X\": X,\n",
        "                \"y\": y,\n",
        "                \"label_map\": label_map,\n",
        "                \"features\": features,\n",
        "                \"feature_names\": feature_names,\n",
        "                \"splits\": splits,\n",
        "            }\n",
        "\n",
        "            self.pipeline_status[\"data_loading\"] = True\n",
        "            print(\"--  Data loading completed successfully!\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Data loading failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_traditional_ml(self):\n",
        "        \"\"\"Step 2: Traditional Machine Learning\"\"\"\n",
        "        self.print_step_header(2, \"TRADITIONAL MACHINE LEARNING\")\n",
        "\n",
        "        try:\n",
        "            if not self.pipeline_status[\"data_loading\"]:\n",
        "                raise ValueError(\"Data not loaded - run Step 1 first\")\n",
        "\n",
        "            data = self.results[\"data\"]\n",
        "            splits = data[\"splits\"]\n",
        "\n",
        "            print(\"--  Initializing traditional ML models...\")\n",
        "            traditional_ml = TraditionalML()\n",
        "\n",
        "            print(\"--  Training Logistic Regression, Random Forest, and SVM...\")\n",
        "            traditional_results = traditional_ml.train_and_evaluate(\n",
        "                splits[\"traditional\"][\"X_train\"],\n",
        "                splits[\"traditional\"][\"X_test\"],\n",
        "                splits[\"traditional\"][\"y_train\"],\n",
        "                splits[\"traditional\"][\"y_test\"],\n",
        "            )\n",
        "\n",
        "            if not traditional_results:\n",
        "                raise ValueError(\"Traditional ML training returned no results\")\n",
        "\n",
        "            print(\"-- Saving traditional ML results...\")\n",
        "            traditional_ml.save_results()\n",
        "\n",
        "            self.results[\"traditional_ml\"] = traditional_results\n",
        "            self.pipeline_status[\"traditional_ml\"] = True\n",
        "\n",
        "            print(\"--  Traditional ML training completed successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Traditional ML training failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_deep_learning(self):\n",
        "        \"\"\"Step 3: Deep Learning Models\"\"\"\n",
        "        self.print_step_header(3, \"DEEP LEARNING MODELS\")\n",
        "\n",
        "        try:\n",
        "            if not TF_AVAILABLE:\n",
        "                print(\"âš ï¸  Deep learning libraries not available. Skipping deep learning step...\")\n",
        "                self.pipeline_status[\"deep_learning\"] = True\n",
        "                return True\n",
        "\n",
        "            if not self.pipeline_status[\"data_loading\"]:\n",
        "                raise ValueError(\"Data not loaded - run Step 1 first\")\n",
        "\n",
        "            data = self.results[\"data\"]\n",
        "            splits = data[\"splits\"]\n",
        "\n",
        "            print(\"ðŸ§  Initializing deep learning models...\")\n",
        "            dl_models = DeepLearningModels()\n",
        "\n",
        "            print(\"--  Training CNN and LSTM models...\")\n",
        "            dl_results = dl_models.train_models(\n",
        "                splits[\"deep_learning\"][\"X_train\"],\n",
        "                splits[\"deep_learning\"][\"X_val\"],\n",
        "                splits[\"deep_learning\"][\"X_test\"],\n",
        "                splits[\"deep_learning\"][\"y_train\"],\n",
        "                splits[\"deep_learning\"][\"y_val\"],\n",
        "                splits[\"deep_learning\"][\"y_test\"],\n",
        "            )\n",
        "\n",
        "            print(\"-- Saving deep learning results...\")\n",
        "            dl_models.save_results()\n",
        "\n",
        "            self.results[\"deep_learning\"] = dl_results\n",
        "            self.pipeline_status[\"deep_learning\"] = True\n",
        "\n",
        "            print(\"--  Deep learning training completed successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Deep learning training failed: {e}\")\n",
        "            # Don't fail the whole pipeline if DL fails\n",
        "            self.pipeline_status[\"deep_learning\"] = True\n",
        "            return True\n",
        "\n",
        "    def generate_results_summary(self):\n",
        "        \"\"\"Step 4: Results Analysis and Summary\"\"\"\n",
        "        self.print_step_header(4, \"RESULTS ANALYSIS AND SUMMARY\")\n",
        "\n",
        "        try:\n",
        "            print(\"ðŸ“ˆ Generating comprehensive results summary...\")\n",
        "\n",
        "            # Load and combine all available results\n",
        "            all_results = []\n",
        "\n",
        "            # Traditional ML results\n",
        "            try:\n",
        "                trad_results = pd.read_csv(\"results/traditional_ml_results.csv\")\n",
        "                all_results.append(trad_results)\n",
        "                print(\"--  Loaded traditional ML results\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"âš ï¸  Traditional ML results not found\")\n",
        "\n",
        "            # Deep Learning results\n",
        "            try:\n",
        "                dl_results = pd.read_csv(\"results/deep_learning_results.csv\")\n",
        "                all_results.append(dl_results)\n",
        "                print(\"--  Loaded deep learning results\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"âš ï¸  Deep learning results not found\")\n",
        "\n",
        "            if not all_results:\n",
        "                print(\"--  No results found to generate summary\")\n",
        "                return False\n",
        "\n",
        "            # Combine all results\n",
        "            combined_results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "            # Generate comprehensive summary\n",
        "            self._print_detailed_summary(combined_results)\n",
        "\n",
        "            # Save combined results\n",
        "            combined_results.to_csv(\"results/combined_results.csv\", index=False)\n",
        "\n",
        "            self.pipeline_status[\"visualization\"] = True\n",
        "            print(\"--  Results summary completed successfully!\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"--  Results summary generation failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _print_detailed_summary(self, results_df):\n",
        "        \"\"\"Print detailed results summary\"\"\"\n",
        "        print(\"\\n\" + \"--  COMPREHENSIVE PERFORMANCE REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Sort by accuracy\n",
        "        ranked_results = results_df.sort_values(\"Accuracy\", ascending=False)\n",
        "\n",
        "        print(\"\\nðŸ† MODEL RANKINGS:\")\n",
        "        print(\"-\" * 40)\n",
        "        for idx, (_, row) in enumerate(ranked_results.iterrows(), 1):\n",
        "            accuracy_percent = row[\"Accuracy\"] * 100\n",
        "            stars = \"â­\" * min(5, int(row[\"Accuracy\"] * 10 // 2))\n",
        "            rank_icon = [\"ðŸ¥‡\", \"ðŸ¥ˆ\", \"ðŸ¥‰\"][idx - 1] if idx <= 3 else f\"{idx:2d}\"\n",
        "\n",
        "            print(f\"   {rank_icon} {row['Model']:20} {accuracy_percent:6.2f}% {stars}\")\n",
        "\n",
        "        # Best model\n",
        "        best_model = ranked_results.iloc[0]\n",
        "        print(f\"\\n--  BEST PERFORMING MODEL:\")\n",
        "        print(f\"   Model:    {best_model['Model']}\")\n",
        "        print(\n",
        "            f\"   Accuracy: {best_model['Accuracy']:.4f} ({best_model['Accuracy']*100:.2f}%)\"\n",
        "        )\n",
        "        print(f\"   Type:     {best_model['Type']}\")\n",
        "\n",
        "        # Statistics\n",
        "        print(f\"\\nðŸ“ˆ PERFORMANCE STATISTICS:\")\n",
        "        print(f\"   Total Models:    {len(results_df)}\")\n",
        "        print(f\"   Average Accuracy: {results_df['Accuracy'].mean():.4f}\")\n",
        "        print(f\"   Best Accuracy:    {results_df['Accuracy'].max():.4f}\")\n",
        "        print(f\"   Worst Accuracy:   {results_df['Accuracy'].min():.4f}\")\n",
        "\n",
        "        # Model type breakdown\n",
        "        model_types = results_df[\"Type\"].value_counts()\n",
        "        print(f\"\\n--  MODEL TYPE BREAKDOWN:\")\n",
        "        for model_type, count in model_types.items():\n",
        "            type_accuracy = results_df[results_df[\"Type\"] == model_type][\n",
        "                \"Accuracy\"\n",
        "            ].mean()\n",
        "            print(f\"   {model_type:20} {count:2d} models, avg: {type_accuracy:.4f}\")\n",
        "\n",
        "    def calculate_execution_time(self):\n",
        "        \"\"\"Calculate and format execution time\"\"\"\n",
        "        if self.start_time:\n",
        "            end_time = time.time()\n",
        "            total_seconds = end_time - self.start_time\n",
        "            minutes = int(total_seconds // 60)\n",
        "            seconds = int(total_seconds % 60)\n",
        "            return minutes, seconds\n",
        "        return 0, 0\n",
        "\n",
        "    def print_final_summary(self):\n",
        "        \"\"\"Print final pipeline summary\"\"\"\n",
        "        minutes, seconds = self.calculate_execution_time()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ðŸŽ‰ PIPELINE EXECUTION COMPLETED!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Pipeline status\n",
        "        print(\"\\n--  PIPELINE STATUS:\")\n",
        "        for step, status in self.pipeline_status.items():\n",
        "            status_icon = \"-- \" if status else \"-- \"\n",
        "            step_name = step.replace(\"_\", \" \").title()\n",
        "            print(f\"   {status_icon} {step_name}\")\n",
        "\n",
        "        # Data summary\n",
        "        if self.pipeline_status[\"data_loading\"]:\n",
        "            data = self.results[\"data\"]\n",
        "            print(f\"\\n--  DATA SUMMARY:\")\n",
        "            print(f\"   Samples: {len(data['X'])}\")\n",
        "            print(f\"   Classes: {len(data['label_map'])}\")\n",
        "            print(f\"   Signal Length: {data['X'].shape[1]}\")\n",
        "\n",
        "        # Results summary\n",
        "        print(f\"\\nâ±ï¸  EXECUTION TIME:\")\n",
        "        print(f\"   Total: {minutes} minutes {seconds} seconds\")\n",
        "\n",
        "        print(f\"\\n-- OUTPUTS GENERATED:\")\n",
        "        output_dirs = [\"results\", \"models\", \"data/processed\"]\n",
        "        for dir_path in output_dirs:\n",
        "            if os.path.exists(dir_path) and os.listdir(dir_path):\n",
        "                file_count = len(os.listdir(dir_path))\n",
        "                print(f\"   {dir_path}: {file_count} files\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Execute the complete pipeline\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        self.print_header()\n",
        "\n",
        "        # Validate environment first\n",
        "        if not self.validate_environment():\n",
        "            print(\"--  Environment validation failed. Please check setup.\")\n",
        "            return False\n",
        "\n",
        "        # Execute pipeline steps\n",
        "        steps = [\n",
        "            self.load_data,\n",
        "            self.train_traditional_ml,\n",
        "            self.train_deep_learning,\n",
        "            self.generate_results_summary,\n",
        "        ]\n",
        "\n",
        "        successful_steps = 0\n",
        "        for step_func in steps:\n",
        "            try:\n",
        "                if step_func():\n",
        "                    successful_steps += 1\n",
        "            except Exception as e:\n",
        "                print(f\"--  Step failed with exception: {e}\")\n",
        "                traceback.print_exc()\n",
        "                # Continue with next step instead of failing completely\n",
        "\n",
        "        # Final summary\n",
        "        self.print_final_summary()\n",
        "\n",
        "        # Success criteria - at least data loading and traditional ML should work\n",
        "        if successful_steps >= 2:\n",
        "            print(\"--  Pipeline completed with acceptable success!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âš ï¸  Pipeline completed with limited success.\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hLEb15M4iVO_",
        "outputId": "a1f89586-1684-4280-90bd-f2fbe7a85514"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import traceback\n",
        "from plot_results import main\n",
        "\n",
        "try:\n",
        "  pipeline = PipelineExecutor()\n",
        "  success = pipeline.run_pipeline()\n",
        "  if success:\n",
        "      print(\"\\n--  Pipeline executed successfully!\")\n",
        "      main()  # Call visualization only if pipeline succeeded\n",
        "  else:\n",
        "      print(\"\\n--  Pipeline executed with some issues. Check logs above.\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\n\\n--  Pipeline interrupted by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--  Unexpected pipeline failure: {e}\")\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
